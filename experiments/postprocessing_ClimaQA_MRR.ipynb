{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import load_dataset\n",
    "import pandas as pd\n",
    "            \n",
    "train, test, dev = load_dataset(\"climaQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test = test[test['label'] == 1].head(100)\n",
    "queries = test['query'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"tcalamai/climaQA_42_distilRoBERTa\", token=os.environ['HUB_TOKEN'],  padding=\"max_length\", truncation=True, max_length=512, device_map=\"auto\")\n",
    "\n",
    "inputs_lists = []\n",
    "label_lists = []\n",
    "\n",
    "for i, r in true_test.iterrows():\n",
    "    inputs_lists += [\n",
    "        [{'text': r['text'], 'text_pair': q} for q in queries]\n",
    "        ]\n",
    "    label_lists += [r['query']]\n",
    "\n",
    "outputs_lists = []\n",
    "\n",
    "for input_list in tqdm(inputs_lists, desc=\"Running predictions\"):\n",
    "    outputs_lists.append(pipe(input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "for output_list, query in zip(outputs_lists, label_lists):\n",
    "\n",
    "    scoring_df = pd.DataFrame(output_list)\n",
    "    scoring_df['queries'] = queries\n",
    "\n",
    "    scoring_df.loc[scoring_df['label'] == 0, \"score\"] = 1-scoring_df.loc[scoring_df['label'] == 0, \"score\"]\n",
    "\n",
    "    scoring_df = scoring_df.sort_values(['label', 'score'], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "    ranks += [1+scoring_df[scoring_df['queries'] == query].index[0]]\n",
    "\n",
    "ranks = np.array(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "reciprocal_ranking = 1/np.array(ranks)\n",
    "mrr = np.mean(reciprocal_ranking)\n",
    "\n",
    "mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_mrr(ranks):\n",
    "    \"\"\"Calculates the Mean Reciprocal Rank (MRR) given a list of ranks.\"\"\"\n",
    "    return np.mean([1 / rank for rank in ranks if rank > 0])\n",
    "\n",
    "def bootstrap_mrr(ranks, n_iterations=1000, confidence_level=0.95):\n",
    "    \"\"\"Performs bootstrapping to calculate the MRR distribution and confidence interval.\"\"\"\n",
    "    mrr_samples = []\n",
    "    n = len(ranks)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Sample with replacement from the original ranks\n",
    "        sample = np.random.choice(ranks, size=n, replace=True)\n",
    "        mrr_samples.append(calculate_mrr(sample))\n",
    "    \n",
    "    # Calculate the confidence interval\n",
    "    lower_percentile = (1 - confidence_level) / 2\n",
    "    upper_percentile = 1 - lower_percentile\n",
    "    ci_lower = np.percentile(mrr_samples, lower_percentile * 100)\n",
    "    ci_upper = np.percentile(mrr_samples, upper_percentile * 100)\n",
    "    \n",
    "    # Return the bootstrapped MRR samples and confidence interval\n",
    "    return ci_lower, ci_upper\n",
    "\n",
    "# Example usage\n",
    "ranks = [1, 2, 3, 4, 5]  # Replace with your list of ranks\n",
    "n_iterations = 1000  # Number of bootstrap iterations\n",
    "confidence_level = 0.95  # Confidence level for the interval\n",
    "\n",
    "ci_lower, ci_upper = bootstrap_mrr(ranks, n_iterations, confidence_level)\n",
    "print(f\"{int(confidence_level * 100)}% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_ranking = train[train['label']==1]['query'].value_counts(normalize=True).reset_index()\n",
    "\n",
    "map_ranking_distrib = dict()\n",
    "for i, v in enumerate(fixed_ranking['query'].values):\n",
    "    map_ranking_distrib[v] = i+1\n",
    "\n",
    "np.mean(1/test[test['label']==1]['query'].map(map_ranking_distrib))\n",
    "\n",
    "ci_lower, ci_upper = bootstrap_mrr(test[test['label']==1]['query'].map(map_ranking_distrib).values, n_iterations, confidence_level)\n",
    "print(f\"{int(confidence_level * 100)}% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(1/np.random.randint(1, len(test[test['label']==1]['query'].unique())+1, len(test[test['label']==1])))\n",
    "\n",
    "random_ranking = np.random.randint(\n",
    "    1, \n",
    "    len(test[test['label']==1]['query'].unique())+1,\n",
    "    len(test[test['label']==1])\n",
    ")\n",
    "\n",
    "ci_lower, ci_upper = bootstrap_mrr(random_ranking, n_iterations, confidence_level)\n",
    "print(f\"{int(confidence_level * 100)}% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
