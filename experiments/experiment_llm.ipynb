{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T11:03:29.081933400Z",
     "start_time": "2024-09-17T11:03:29.030004200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import httpx\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"openai-key\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    http_client=httpx.Client(proxies=proxies)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def open_ai_chat_completion(messages, model=\"gpt-4o-mini\"):\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    headers = {'content-type': 'application/json', \"Authorization\": \"Bearer f{openai_api_key}\"}\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34744e01c71aabe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T11:03:30.057050800Z",
     "start_time": "2024-09-17T11:03:29.373280200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from src.utils import Generator\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109535c5ffaf66d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Create the prompts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b7cfcd2f442ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1) generate the prompt for classification\n",
    "2) generate the prompt for relation classification\n",
    "3) generate the prompt for multlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22485c3c0a7ebc58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T11:03:32.035341700Z",
     "start_time": "2024-09-17T11:03:31.981547400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSON file\n",
    "with open(os.path.join(\"llm\", \"mappings\", \"task_description.json\"), 'r', encoding='utf-8') as file:\n",
    "    task_descriptions = json.load(file)\n",
    "\n",
    "# Open the JSON file\n",
    "with open(os.path.join(\"llm\", \"mappings\", \"label_annotation.json\"), 'r', encoding='utf-8') as file:\n",
    "    label_readable_mapping = json.load(file)\n",
    "\n",
    "# task_descriptions['climatext_wiki'] = task_descriptions['climatext']\n",
    "# task_descriptions['climatext_10k'] = task_descriptions['climatext']\n",
    "# task_descriptions['climatext_claim'] = task_descriptions['climatext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458ee302be1de5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T11:03:33.474607500Z",
     "start_time": "2024-09-17T11:03:33.373672200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "template=\"\"\"Prompt Template:\n",
    "[STARTPROMPT]\n",
    "You are a text classifier. Your task is to label the provided input based on the criteria outlined below.\n",
    "\n",
    "Task: Describe the classification task clearly here (e.g., determine if a paragraph is [specific] or [non-specific]).\n",
    "\n",
    "Labels and Definitions:\n",
    "\n",
    "    [Label 1]: Include a definition for this label.\n",
    "    [Label 2]: Include a definition for this label.\n",
    "    [Additional Labels]: Include definitions as necessary.\n",
    "\n",
    "Formatting: Your answer should be formatted in this way to ensure consistency:\n",
    "```\n",
    "Label: [Insert Label]\n",
    "Explanation: [Provide a brief, clear explanation justifying the label chosen]\n",
    "```\n",
    "\n",
    "Example of well-formatted answer:\n",
    "```\n",
    "Label: [specific]\n",
    "Explanation: The paragraph provides specific details about a project that the company plans to implement, including measurable goals.\n",
    "```\n",
    "\n",
    "Input text: [[Insert Text Here]]\n",
    "[ENDPROMPT]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e601bd398fe7b8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T11:03:36.552798500Z",
     "start_time": "2024-09-17T11:03:36.439655600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_dict(dict_data, filename):\n",
    "    \"\"\"\n",
    "    Save the dictionary to a JSON file, appending only new keys. Warn if a key is already present and not saved.\n",
    "    \n",
    "    Parameters:\n",
    "    dict_data (dict): The dictionary to save.\n",
    "    filename (str): The name of the file where the dictionary will be saved.\n",
    "    \"\"\"\n",
    "    # Check if file exists, load its content if it does\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            try:\n",
    "                existing_data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = {}\n",
    "    else:\n",
    "        existing_data = {}\n",
    "\n",
    "    # Prepare the data to save\n",
    "    new_data = {}\n",
    "    for key, value in dict_data.items():\n",
    "        if key in existing_data:\n",
    "            print(f\"Warning: Key '{key}' already exists, not saving it.\")\n",
    "        else:\n",
    "            new_data[key] = value\n",
    "\n",
    "    # If there are new keys, update the file\n",
    "    if new_data:\n",
    "        existing_data.update(new_data)\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "        print(f\"New keys saved: {list(new_data.keys())}\")\n",
    "    else:\n",
    "        print(\"No new keys to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf9dc9672fe7dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T10:19:38.793516900Z",
     "start_time": "2024-09-12T10:17:37.989216900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompts = dict() \n",
    "\n",
    "# classification: make sure to use the dataset from ClimaINS_ours\n",
    "for dataset_name in set(generator.dataset_builder.datasets.keys())-{'ClimaINS_ours'}:\n",
    "    print(dataset_name)\n",
    "    description = task_descriptions[dataset_name]['description']\n",
    "    prompt = \"description: \\n\" + description + \"\\n\\n\"\n",
    "    prompt += \"Labels: \\n\"\n",
    "    \n",
    "    for label in task_descriptions[dataset_name]['labels'].keys():\n",
    "        \n",
    "        if dataset_name in label_readable_mapping.keys():\n",
    "            prompt += \"- [\"+label_readable_mapping[dataset_name]['labels'][str(label)] + \"]: \"\n",
    "        else:\n",
    "            prompt += \"- [\"+label + \"]: \"\n",
    "        \n",
    "        prompt += task_descriptions[dataset_name]['labels'][str(label)] + \"\\n\"\n",
    "        \n",
    "    response = open_ai_chat_completion(\n",
    "          model=\"gpt-4o\",\n",
    "          messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Could you create a prompt for LLMs (GPT-4 or Llama 3) to make them behave as a Classifier on the following task : \\n {} \\n\\n Make sure to add formatting instruction so the output can easily be parsed and give an example of well formatted answer. Juste give me the prompt. It should be a prompt for one prediction only (not multiple predictions). For the input sentence add this placeholder: [[Insert Text Here]]. Start the prompt with [STARTPROMPT] and end it with [ENDPROMPT]\\n\\n Here is a template that you should follow:\\n{}\".format(prompt, template)}\n",
    "          ])\n",
    "    \n",
    "    prompts[dataset_name] = response['choices'][0]['message']['content']\n",
    "\n",
    "save_dict(prompts, \"llm/prompts_climatext.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83535e3730a543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T17:21:35.956738200Z",
     "start_time": "2024-09-16T17:21:28.819310100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompts = dict() \n",
    "\n",
    "# relation classification:\n",
    "for dataset_name in ['climateFEVER_evidence', 'climaQA', 'lobbymap_stance']:\n",
    "    print(dataset_name)\n",
    "    description = task_descriptions[dataset_name]['description']\n",
    "    prompt = \"description: \\n\" + description + \"\\n\\n\"\n",
    "    prompt += \"Labels: \\n\"\n",
    "    \n",
    "    for label in task_descriptions[dataset_name]['labels'].keys():\n",
    "        \n",
    "        if dataset_name in label_readable_mapping.keys():\n",
    "            prompt += \"- [\"+label_readable_mapping[dataset_name]['labels'][str(label)] + \"]: \"\n",
    "        else:\n",
    "            prompt += \"- [\"+label + \"]: \"\n",
    "        \n",
    "        prompt += task_descriptions[dataset_name]['labels'][str(label)] + \"\\n\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "          model=\"gpt-4o\",\n",
    "          messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Could you create a prompt for LLMs (GPT-4 or Llama 3) to make them behave as a Classifier on the following task : \\n {} \\n\\n Make sure to add formatting instruction so the output can easily be parsed and give an example of well formatted answer. Juste give me the prompt. It should be a prompt for one prediction only (not multiple predictions). The tasks rely on the relation between 2 texts: a text and a query. For the input sentence add this placeholder: [[Insert Text Here]] and [[Insert Query Here]]. Start the prompt with [STARTPROMPT] and end it with [ENDPROMPT]\\n\\n Here is a template that you should follow:\\n{}\".format(prompt, template)}\n",
    "          ]\n",
    "        )\n",
    "    \n",
    "    prompts[dataset_name] = response.choices[0].message.content\n",
    "\n",
    "save_dict(prompts, \"llm/prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8aec57ef6be7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T10:21:52.007411800Z",
     "start_time": "2024-09-12T10:21:37.304636600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompts = dict() \n",
    "\n",
    "# multilabel classification:\n",
    "for dataset_name in generator.dataset_builder.multilabel_datasets.keys():\n",
    "    print(dataset_name)\n",
    "\n",
    "    # prepare the prompt zero-shot first\n",
    "    # system_prompt = \"You are an annotator for NLP tasks related to climate-change. You will be provided with the description of a tasks. Please follow the instructions.\"\n",
    "    description = task_descriptions[dataset_name]['description']\n",
    "    prompt = \"description: \\n\" + description + \"\\n\\n\"\n",
    "    prompt += \"Labels: \\n\"\n",
    "    \n",
    "    for label in task_descriptions[dataset_name]['labels'].keys():\n",
    "        \n",
    "        if dataset_name in label_readable_mapping.keys():\n",
    "            prompt += \"- [\"+label_readable_mapping[dataset_name]['labels'][str(label)] + \"]: \"\n",
    "        else:\n",
    "            prompt += \"- [\"+label + \"]: \"\n",
    "        \n",
    "        prompt += task_descriptions[dataset_name]['labels'][str(label)] + \"\\n\"\n",
    "        \n",
    "    response = client.chat.completions.create(\n",
    "          model=\"gpt-4o\",\n",
    "          messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Could you create a prompt for LLMs (GPT-4 or Llama 3) to make them behave as a Classifier on the following task : \\n {} \\n\\n Make sure to add formatting instruction so the output can easily be parsed and give an example of well formatted answer. Juste give me the prompt. The task is multilabel. For the input sentence add this placeholder: [[Insert Text Here]]. Start the prompt with [STARTPROMPT] and end it with [ENDPROMPT].\\n\\n Here is a template that you should follow {}\".format(prompt, template)}\n",
    "          ]\n",
    "        )\n",
    "    \n",
    "    prompts[dataset_name] = response.choices[0].message.content\n",
    "\n",
    "save_dict(prompts, \"llm/prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cda7f9",
   "metadata": {},
   "source": [
    "## Create the cot version (inversion between label and explanation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dfd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zero_shot import load_dict\n",
    "import re\n",
    "\n",
    "def invert_labels_explanations(text):\n",
    "    pattern = r\"(Label:.*?\\nExplanation:.*?)\\n\"\n",
    "    def swap_label_explanation(match):\n",
    "        label_explanation = match.group(1).split(\"\\n\")\n",
    "        if len(label_explanation) == 2:\n",
    "            label = label_explanation[0]\n",
    "            explanation = label_explanation[1]\n",
    "            return f\"{explanation}\\n{label}\\n\"\n",
    "        return match.group(1)  # Return original if it does not match the expected format\n",
    "    \n",
    "    swapped_text = re.sub(pattern, swap_label_explanation, text)\n",
    "    \n",
    "    return swapped_text\n",
    "\n",
    "\n",
    "prompts = load_dict(\"llm/prompts.json\")\n",
    "cot_prompts = prompts.copy()\n",
    "\n",
    "for k in cot_prompts.keys():\n",
    "    cot_prompts[k] = invert_labels_explanations(cot_prompts[k])\n",
    "\n",
    "save_dict(cot_prompts, \"llm/prompts_cot.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6eb8809675970",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset_name(dataset_name):\n",
    "    if dataset_name == \"climateFEVER_claim\":\n",
    "        return \"climateFEVER claim (our split)\"\n",
    "    elif dataset_name == \"climateFEVER_claim_agg\":\n",
    "        return \"climateFEVER claim (ours split, aggregated)\"\n",
    "    elif dataset_name == \"climateFEVER_claim_climabench_agg\":\n",
    "        return \"climateFEVER claim (climabench split, aggregated)\"\n",
    "    elif dataset_name == \"climateFEVER_evidence_ours\":\n",
    "        return \"climateFEVER evidence (our split)\"\n",
    "    elif dataset_name == \"climateFEVER_evidence_climabench\":\n",
    "        return \"climateFEVER evidence (climabench split)\"\n",
    "    elif dataset_name == \"netzero_reduction_duplicated\":\n",
    "        return \"Net-Zero/Reduction (with duplicates)\"\n",
    "    elif dataset_name == \"netzero_reduction\":\n",
    "        return \"Net-Zero/Reduction\"\n",
    "    elif dataset_name == \"ClimaINS_ours\":\n",
    "        return \"ClimaINS (our split)\"\n",
    "    elif dataset_name == \"climateFEVER_evidence\":\n",
    "        return \"climateFEVER evidence\"\n",
    "    elif dataset_name == \"climateBUG_data\":\n",
    "        return \"climateBUG-data\"\n",
    "    elif dataset_name == \"climate_commitments_actions\":\n",
    "        return \"Commitments And Actions\"\n",
    "    elif dataset_name == \"climate_detection\":\n",
    "        return \"ClimateBERT's Climate detection\"\n",
    "    elif dataset_name == \"climate_specificity\":\n",
    "        return \"Climate Specificity\"\n",
    "    elif dataset_name == \"climate_sentiment\":\n",
    "        return \"climate sentiment\"\n",
    "    elif dataset_name == \"climate_tcfd_recommendations\":\n",
    "        return \"Climate TCFD recommendations\"\n",
    "    elif dataset_name == \"esgbert_category_water\":\n",
    "        return \"esgbert Water\"\n",
    "    elif dataset_name == \"esgbert_category_forest\":\n",
    "        return \"esgbert Forest\"\n",
    "    elif dataset_name == \"esgbert_category_biodiversity\":\n",
    "        return \"esgbert Biodiversity\"\n",
    "    elif dataset_name == \"esgbert_category_nature\":\n",
    "        return \"esgbert Nature\"\n",
    "    elif dataset_name == \"environmental_claims\":\n",
    "        return \"Environmental Claims\"\n",
    "    elif dataset_name == \"green_claims\":\n",
    "        return \"Green Claims\"\n",
    "    elif dataset_name == \"green_claims_3\":\n",
    "        return \"Implicit/Explicit Green Claims\"\n",
    "    elif dataset_name == \"contrarian_claims\":\n",
    "        return \"CC-Contrarian Claims\"\n",
    "    elif dataset_name == \"esgbert_e\":\n",
    "        return \"ESGBERT E\"\n",
    "    elif dataset_name == \"esgbert_s\":\n",
    "        return \"ESGBERT S\"\n",
    "    elif dataset_name == \"esgbert_g\":\n",
    "        return \"ESGBERT G\"\n",
    "    elif dataset_name == \"gw_stance_detection\":\n",
    "        return \"Global-Warming Stance (GWSD)\"\n",
    "    elif dataset_name == \"sustainable_signals_review\":\n",
    "        return \"SUSTAINABLESIGNALS reviews\"\n",
    "    elif dataset_name == \"lobbymap_stance\":\n",
    "        return \"LobbyMap (Stance)\"\n",
    "    elif dataset_name == \"lobbymap_query\":\n",
    "        return \"LobbyMap (Query)\"\n",
    "    elif dataset_name == \"lobbymap_pages\":\n",
    "        return \"LobbyMap (Pages)\"\n",
    "    elif dataset_name == \"lobbymap_query_p\":\n",
    "        return \"LobbyMap (Page)\"\n",
    "    elif dataset_name == \"lobbymap_query_stance\":\n",
    "        return \"LobbyMap (Stance)\"\n",
    "    elif dataset_name == \"climatext\":\n",
    "        return \"climatext (Wiki-doc)\"\n",
    "    elif dataset_name == \"climatext_wiki\":\n",
    "        return \"climatext (Wikipedia)\"\n",
    "    elif dataset_name == \"climatext_10k\":\n",
    "        return \"climatext (10k)\"\n",
    "    elif dataset_name == \"climatext_claim\":\n",
    "        return \"climatext (claim)\"\n",
    "    elif dataset_name == \"esgbert_action500\":\n",
    "        return \"ESGBERT action500\"\n",
    "    else:\n",
    "        return dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530578a8c48de32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T11:03:42.966122500Z",
     "start_time": "2024-09-17T11:03:42.903182600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Test set to use as a whole :\n",
    "copy_full_test_set = [\n",
    "    'netzero_reduction',\n",
    "    'climate_specificity',\n",
    "    'climate_sentiment',\n",
    "    'climate_commitments_actions',\n",
    "    'climate_detection',\n",
    "    'climate_tcfd_recommendations',\n",
    "    'environmental_claims',\n",
    "    'climateFEVER_claim',\n",
    "    'sustainable_signals_review',\n",
    "    'esgbert_e',\n",
    "    'esgbert_s',\n",
    "    'esgbert_g',\n",
    "    'esgbert_action500',\n",
    "    'esgbert_category_water',\n",
    "    'esgbert_category_forest',\n",
    "    'esgbert_category_biodiversity',\n",
    "    'esgbert_category_nature',\n",
    "    'green_claims',\n",
    "    'green_claims_3',\n",
    "    'climateStance',\n",
    "    'climateEng',\n",
    "    'gw_stance_detection',\n",
    "    'climateFEVER_evidence'\n",
    "    'logicClimate',\n",
    "    'ClimaINS_ours',\n",
    "]\n",
    "\n",
    "not_done = ['ClimaINS', \"climateFEVER_evidence_climabench\"]\n",
    "\n",
    "# Test set to limit to 1000 samples:\n",
    "large_dataset_to_downsample = [\n",
    "    'sciDCC',\n",
    "    'contrarian_claims',\n",
    "    'lobbymap_stance',\n",
    "    'lobbymap_query',\n",
    "    'lobbymap_pages',\n",
    "    'climaQA',\n",
    "    'climatext',\n",
    "    'ClimaTOPIC',\n",
    "    'climateBUG_data'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cot = True\n",
    "use_gpt4 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0b3babc575a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T11:03:46.599041600Z",
     "start_time": "2024-09-17T11:03:46.520621900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from zero_shot import load_dict, extract_prompt, update_question, map_lobbymap_stance, prepare_content\n",
    "\n",
    "\n",
    "# Open the JSON file\n",
    "with open(os.path.join(\"llm\", \"mappings\", \"task_description.json\"), 'r', encoding='utf-8') as file:\n",
    "    task_descriptions = json.load(file)\n",
    "\n",
    "# Open the JSON file\n",
    "with open(os.path.join(\"llm\", \"mappings\", \"label_annotation.json\"), 'r', encoding='utf-8') as file:\n",
    "    label_readable_mapping = json.load(file)\n",
    "\n",
    "prompts = load_dict(\"llm/prompts_cot.json\") if use_cot else load_dict(\"llm/prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b6a4f3c7edef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:27:36.898774900Z",
     "start_time": "2024-09-17T13:27:36.859778100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def check_prompt_format(prompt):\n",
    "    # Define a combined regex pattern for Label followed by Explanation\n",
    "    combined_pattern = r\"Explanation:\\s*.*\\nLabel:\\s*\\[.*?\\]\"\n",
    "\n",
    "    # Search for the combined pattern in the prompt\n",
    "    combined_match = re.search(combined_pattern, prompt)\n",
    "\n",
    "    # Check if both Label and Explanation exist in the correct format\n",
    "    if combined_match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "for dataset_name in prompts.keys():    \n",
    "    if not check_prompt_format(prompts[dataset_name]):\n",
    "        print(\"#\"*15)\n",
    "        print(dataset_name, \"Wrongly formatted '''label: explaination:''' instruct\")\n",
    "        print(\"#\"*15)\n",
    "        print(prompts[dataset_name])\n",
    "        print(\"#\"*15, \"\\n\\n\")\n",
    "    elif \"[[Insert Text Here]]\" not in prompts[dataset_name]:\n",
    "        print(\"#\"*15)\n",
    "        print(dataset_name, \"Missing [[Insert Text Here]]\")\n",
    "        print(\"#\"*15)\n",
    "        print(prompts[dataset_name])\n",
    "        print(\"#\"*15, \"\\n\\n\")\n",
    "    elif (\"[[Insert Query Here]]\" not in prompts[dataset_name]) and (dataset_name in ['climateFEVER_evidence', 'climaQA', 'lobbymap_stance']):\n",
    "        print(\"#\"*15)\n",
    "        print(dataset_name, \"Missing [[Insert Query Here]]\")\n",
    "        print(\"#\"*15)\n",
    "        print(prompts[dataset_name])\n",
    "        print(\"#\"*15, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb56a62",
   "metadata": {},
   "source": [
    "### initialize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_create(file_name, api_key):\n",
    "  url = 'https://api.openai.com/v1/files'\n",
    "  headers = {\n",
    "      'Authorization': f'Bearer {api_key}'\n",
    "  }\n",
    "  data = {\n",
    "      'purpose': 'batch'\n",
    "  }\n",
    "\n",
    "  # Open the file and send the request\n",
    "  with open(file_name, 'rb') as f:\n",
    "      files = {'file': (file_name, f)}\n",
    "      response = requests.post(url, headers=headers, files=files, data=data)\n",
    "\n",
    "  # Check for a successful response\n",
    "  if response.status_code == 200:\n",
    "      print(\"File uploaded successfully!\")\n",
    "      return response.json()\n",
    "  else:\n",
    "      print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce957c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_create(file_id, api_key):\n",
    "    # Define the API endpoint for creating a batch job\n",
    "    url = 'https://api.openai.com/v1/batches'\n",
    "\n",
    "    # Define the headers\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Define the data for the batch job\n",
    "    data = {\n",
    "        \"input_file_id\": file_id,  # Replace with your actual file ID\n",
    "        \"endpoint\": \"/v1/chat/completions\",\n",
    "        \"completion_window\": \"24h\"\n",
    "    }\n",
    "\n",
    "    # Send the POST request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    # Check for a successful response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Batch job created successfully!\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_retrieve(batch_id, api_key):\n",
    "    url = f'https://api.openai.com/v1/batches/{batch_id}'\n",
    "\n",
    "    # Set up the headers with your API key\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check for a successful response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Batch details retrieved successfully!\")\n",
    "        return response.json()  # Return the response data\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_content(file_id, api_key, output_file):\n",
    "    # Define the API endpoint for retrieving the file content\n",
    "    url = f'https://api.openai.com/v1/files/{file_id}/content'\n",
    "\n",
    "    # Set up the headers with your API key\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "\n",
    "    # Check for a successful response\n",
    "    if response.status_code == 200:\n",
    "        # Open the output file in write mode and save the content\n",
    "        with open(output_file, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"File content saved to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf589be",
   "metadata": {},
   "source": [
    "## Launch batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b56fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_run = ['climatext_10k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4d1264b84ca5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:28:30.279791900Z",
     "start_time": "2024-09-17T12:28:28.205884700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_jobs = dict()\n",
    "\n",
    "for dataset_name in dataset_to_run:\n",
    "    print(dataset_name)\n",
    "    \n",
    "    # read dataset\n",
    "    if dataset_name == \"ClimaINS.pkl\":\n",
    "        continue\n",
    "    \n",
    "    if dataset_name in [\"lobbymap_stance\", \"lobbymap_query\"]:\n",
    "        prompts[dataset_name+\"_origin\"] = prompts[dataset_name]\n",
    "        dataset_name = dataset_name+\"_origin\"\n",
    "\n",
    "    if dataset_name in done:\n",
    "        print(\"pass\")\n",
    "        continue\n",
    "    \n",
    "    test = pd.read_parquet(os.path.join(\"data\", \"llm_green_nlp_tasks\", f\"{dataset_name}.pkl\"))\n",
    "\n",
    "    if \"clean_text\" not in test.columns:\n",
    "        test['clean_text'] = test['text']\n",
    "        \n",
    "    tasks = []\n",
    "    \n",
    "    for index, row in test.iterrows():        \n",
    "        task = {\n",
    "            \"custom_id\": f\"task-{index}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o\" if use_gpt4 else \"gpt-4o-mini\",\n",
    "                \"temperature\": 0.1,\n",
    "                \"seed\": 42,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an AI annotator for NLP tasks related to climate-change. You will be provided with the description of a tasks. Please follow the instructions.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prepare_content(row, dataset_name, task_descriptions, prompts)\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        tasks.append(task)\n",
    "   \n",
    "    file_name = f\"llm/tasks/{dataset_name}.json\"\n",
    "    \n",
    "    with open(file_name, 'w') as file:\n",
    "        for obj in tasks:\n",
    "            file.write(json.dumps(obj) + '\\n')\n",
    "            \n",
    "    batch_file = files_create(file_name=file_name, api_key=openai_api_key)\n",
    "    batch_job = batches_create(file_id=batch_file['id'], api_key=openai_api_key)\n",
    "    batch_jobs[dataset_name] = batch_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"batch.json\", \"w\") as json_file:\n",
    "    json.dump(batch_jobs, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c77eb5c7d583dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:39:07.498359900Z",
     "start_time": "2024-09-17T13:39:05.892464600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ### LOBBYMAP (one shot)\n",
    "\n",
    "# test = pd.read_csv('llm/dataset/lobbymap.csv', sep=\"\\t\")\n",
    "# dataset_name = \"lobbymap\"\n",
    "\n",
    "# batch_jobs = dict()\n",
    "\n",
    "# tasks = []\n",
    "\n",
    "# for index, row in test.iterrows():        \n",
    "#     task = {\n",
    "#         \"custom_id\": f\"task-{index}\",\n",
    "#         \"method\": \"POST\",\n",
    "#         \"url\": \"/v1/chat/completions\",\n",
    "#         \"body\": {\n",
    "#             # This is what you would have in your Chat Completions API call\n",
    "#             \"model\": \"gpt-4o-mini\",\n",
    "#             \"temperature\": 0.1,\n",
    "#             \"seed\": 42,\n",
    "#             \"messages\": [\n",
    "#                 {\n",
    "#                     \"role\": \"system\",\n",
    "#                     \"content\": \"You are an AI annotator for NLP tasks related to climate-change. You will be provided with the description of a tasks. Please follow the instructions.\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"role\": \"user\",\n",
    "#                     \"content\": prepare_content(row, dataset_name, task_descriptions, prompts)\n",
    "#                 }\n",
    "#             ],\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     tasks.append(task)\n",
    "\n",
    "# # Creating the file\n",
    "\n",
    "# # file_name = f\"llm/tasks/{dataset_name}.json\"\n",
    "\n",
    "# # with open(file_name, 'w') as file:\n",
    "# #     for obj in tasks:\n",
    "# #         file.write(json.dumps(obj) + '\\n')\n",
    "    \n",
    "# # batch_file = client.files.create(\n",
    "# #     file=open(file_name, \"rb\"),\n",
    "# #     purpose=\"batch\"\n",
    "# #     )\n",
    "\n",
    "# # batch_job = client.batches.create(\n",
    "# #     input_file_id=batch_file.id,\n",
    "# #     endpoint=\"/v1/chat/completions\",\n",
    "# #     completion_window=\"24h\"\n",
    "# #     )\n",
    "\n",
    "# # batch_jobs[dataset_name] = batch_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f34d7fd346615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:39:27.131679500Z",
     "start_time": "2024-09-17T13:39:26.864362900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for dataset_name in batch_jobs.keys():\n",
    "    print(dataset_name, \"-\", batches_retrieve(batch_jobs[dataset_name]['id'], api_key=openai_api_key)['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810470cf3b6a7aa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:39:33.926645800Z",
     "start_time": "2024-09-17T13:39:32.519960400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for dataset_name in batch_jobs.keys():\n",
    "    batch_job = batches_retrieve(batch_jobs[dataset_name]['id'], api_key=openai_api_key)\n",
    "    if batch_job['status'] == \"completed\":\n",
    "        \n",
    "        result_file_id = batch_job['output_file_id']\n",
    "        # result_file_name = \n",
    "        # result_file_name = f\"llm/outputs/gpt-4o/{dataset_name}.jsonl\" if use_gpt4 else f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "        result_file_name = f\"llm/outputs/cot_full/{dataset_name}.jsonl\" if use_cot else f\"llm/outputs/full/{dataset_name}.jsonl\" \n",
    "\n",
    "        download_file_content(result_file_id, openai_api_key, result_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7e2b9",
   "metadata": {},
   "source": [
    "## Compute performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# done = [dataset_name for dataset_name in batch_jobs.keys()]\n",
    "done = ['climatext_10k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58816ef16914d762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:29:56.927832Z",
     "start_time": "2024-09-17T13:29:56.796496100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_label_explanation(text):\n",
    "    try:\n",
    "        # Regular expression to extract Label and Explanation\n",
    "        label_pattern = r'Label:\\s*(.*)'\n",
    "        explanation_pattern = r'Explanation:\\s*(.*)'\n",
    "\n",
    "        # Find the label\n",
    "        label_match = re.search(label_pattern, text)\n",
    "        label = label_match.group(1) if label_match else None\n",
    "\n",
    "        # Find the explanation\n",
    "        explanation_match = re.search(explanation_pattern, text, re.DOTALL)\n",
    "        explanation = explanation_match.group(1).strip() if explanation_match else None\n",
    "        \n",
    "        label = label.replace('[', \"\").replace(']', \"\").strip()\n",
    "\n",
    "        label = label.replace('Climate solutions won’t work, Climate policies are harmful', \"Climate solutions won’t work, Climate policies (mitigation or adaptation) are harmful\")\n",
    "        label = label.replace('Climate solutions won’t work, One country is negligible', 'Climate solutions won’t work, Climate policies are ineffective/flawed')\n",
    "\n",
    "    except Exception as e:\n",
    "        label = None\n",
    "        explanation = e\n",
    "\n",
    "    return label, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c7c32acaccfbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:29:58.400736800Z",
     "start_time": "2024-09-17T13:29:58.323982Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from src.logger import bootstrap_confidence_interval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2a76c00af973d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:29:59.058624700Z",
     "start_time": "2024-09-17T13:29:59.043364400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "label_readable_mapping['lobbymap_pages'] = {'labels': \n",
    "                                                {\n",
    "                                                    '1': 'The page contains one or more evidence about the stance of the company regarding any of the policy mentioned above',\n",
    "                                                    '0': 'The page does not contain evidence about the stance of the company regarding any of the policy'\n",
    "                                                }\n",
    "                                            }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9954684bf43f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:30:01.698633900Z",
     "start_time": "2024-09-17T13:30:01.618929200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if use_cot:\n",
    "    perf_file_path = \"experiment_results/performances/performances_cot_llm.csv\"\n",
    "else:\n",
    "    perf_file_path = \"experiment_results/performances/performances_llm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda49050a51123",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:29:34.850055Z",
     "start_time": "2024-09-17T12:29:34.545668900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "model_type = \"gpt-4o\" if use_gpt4 else \"gpt-4o-mini\"\n",
    "performance_type = \"f1_score\"\n",
    "\n",
    "if os.path.exists(perf_file_path):\n",
    "    performances = pd.read_csv(perf_file_path)\n",
    "else:\n",
    "    performances = pd.DataFrame()\n",
    "\n",
    "# set(prompts.keys())-{\"lobbymap_query\", \"logicClimate\"}\n",
    "for dataset_name in batch_jobs.keys():#dataset_to_run:\n",
    "    \n",
    "    # Loading data from saved file\n",
    "    results = []\n",
    "    #result_file_name = f\"llm/outputs/gpt-4o/{dataset_name}.jsonl\" if use_gpt4 else f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "    result_file_name = f\"llm/outputs/cot_full/{dataset_name}.jsonl\" if use_cot else f\"llm/outputs/full/{dataset_name}.jsonl\"\n",
    "\n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parsing the JSON string into a dict and appending to the list of results\n",
    "            json_object = json.loads(line.strip())\n",
    "            results.append(json_object[\"response\"]['body'][\"choices\"][0]['message']['content'])\n",
    "    \n",
    "    labels = []\n",
    "    explainations = []\n",
    "    \n",
    "    for result in results:\n",
    "        label, explanation = parse_label_explanation(result)\n",
    "        labels += [label]\n",
    "        explainations += [explanation]\n",
    "    \n",
    "    # test = pd.read_parquet(os.path.join(\"doccano\", \"random\", \"parquet\", f\"{dataset_name}.pkl\"))\n",
    "    test = pd.read_parquet(os.path.join(\"data\", \"llm_green_nlp_tasks\", f\"{dataset_name}.pkl\"))\n",
    "        \n",
    "    if dataset_name == \"lobbymap_pages\":\n",
    "        test['label'] = 1 * test['label']\n",
    "\n",
    "    test['gpt-4o-mini_label'] = labels\n",
    "    test['gpt-4o-mini_explanation'] = explainations\n",
    "\n",
    "    if dataset_name == \"lobbymap_query_origin\":\n",
    "        test = test[test['query'].astype(str) != \"[None]\"]\n",
    "        dataset_name = \"lobbymap_query\"\n",
    "    if dataset_name == \"lobbymap_stance_origin\":\n",
    "        test = test[test['query'].astype(str) != \"None\"]\n",
    "        test.rename(columns={'stance':'label'}, inplace=True)\n",
    "        dataset_name = \"lobbymap_stance\"\n",
    "    \n",
    "    print(dataset_name)\n",
    "    \n",
    "    if dataset_name in [\"logicClimate\", \"lobbymap_query\"]:\n",
    "        if dataset_name == \"logicClimate\":\n",
    "            y_true = test['label'].apply(literal_eval)\n",
    "            y_pred = test['gpt-4o-mini_label'].apply(lambda x: x.split(\",\"))\n",
    "        elif dataset_name == \"lobbymap_query\":\n",
    "            y_true = test['query'].apply(lambda x: [map_lobbymap_stance[e] for e in x])\n",
    "            y_pred = test['gpt-4o-mini_label'].apply(lambda x: [e.strip() for e in x.split(\",\")])   \n",
    "        \n",
    "        # Initialize the MultiLabelBinarizer\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        \n",
    "        # Fit the binarizer and transform the labels\n",
    "        y_true_binarized = mlb.fit_transform(y_true)\n",
    "        y_pred_binarized = mlb.transform(y_pred)\n",
    "        \n",
    "        report = classification_report(\n",
    "                    y_pred=y_pred_binarized, \n",
    "                    y_true=y_true_binarized,\n",
    "                    target_names=mlb.classes_,\n",
    "                    zero_division=0.0,\n",
    "                    output_dict=True \n",
    "                )\n",
    "        \n",
    "        f1_lower, f1_upper = bootstrap_confidence_interval(y_pred=y_pred_binarized, y_true=y_true_binarized, num_bootstrap_samples=100)\n",
    "        print(report['macro avg']['f1-score'], f1_lower, f1_upper)\n",
    "        \n",
    "    else:\n",
    "        if dataset_name in label_readable_mapping:\n",
    "            label2id = {v.lower(): k for k, v in label_readable_mapping[dataset_name]['labels'].items()}\n",
    "\n",
    "            if dataset_name == 'contrarian_claims':\n",
    "                # Split the keys of label2id based on \",\" and create a new mapping\n",
    "                new_label2id = {}\n",
    "                for key, value in label2id.items():\n",
    "                    key_parts = [part.strip().lower() for part in key.split(',')]\n",
    "                    new_label2id[key_parts[-1]] = value\n",
    "\n",
    "                # Function to map and replace labels in the dataframe\n",
    "                def map_labels(label):\n",
    "                    parts = [part.strip().lower() for part in label.split(',')]\n",
    "                    return parts[-1]\n",
    "                    \n",
    "                test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].apply(map_labels)\n",
    "                test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].str.lower().map(new_label2id)\n",
    "            else:\n",
    "                test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].str.lower().map(label2id)\n",
    "\n",
    "        report = classification_report(\n",
    "                y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "                y_true=test['label'].astype(str),\n",
    "                zero_division=0.0,\n",
    "                output_dict=True            \n",
    "            )\n",
    "        \n",
    "        f1_lower, f1_upper = bootstrap_confidence_interval(y_pred=test['gpt-4o-mini_label'].astype(str), y_true=test['label'].astype(str), num_bootstrap_samples=1000)\n",
    "        print(report['macro avg']['f1-score'], f1_lower, f1_upper)\n",
    "        \n",
    "    if ('samples avg' in report.keys()) and ('accuracy' not in report.keys()):\n",
    "        report['accuracy'] = report['samples avg']['f1-score']\n",
    "        \n",
    "    new_row = pd.DataFrame({\n",
    "        'dataset_name': [dataset_name],\n",
    "        'model_type': [model_type],\n",
    "        'performance': [report['macro avg']['f1-score']],\n",
    "        'performance_type': [performance_type],\n",
    "        'n_labels': [np.nan],\n",
    "        'seed': [42],\n",
    "        \"f1_upper\": [f1_upper],\n",
    "        \"f1_lower\": [f1_lower],\n",
    "        \"n_epoch\": [np.nan],\n",
    "        \"precision\": [report['macro avg']['precision']],\n",
    "        \"recall\": [report['macro avg']['recall']],\n",
    "        \"weighted_f1\": [report['weighted avg']['f1-score']],\n",
    "        \"accuracy\": [report['accuracy']]\n",
    "    })\n",
    "    performances = pd.concat([performances, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c136306",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances.to_csv(perf_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741295e50c01c44",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Performance explorer per datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8609f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "performances = pd.read_csv(perf_file_path)\n",
    "performances.sort_values(by=['performance'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb79d79472be38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T09:44:48.644253200Z",
     "start_time": "2024-09-17T09:44:48.561730Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def find_errors(dataset_name, mode=\"mini\"):\n",
    "        \n",
    "    # Loading data from saved file\n",
    "    results = []\n",
    "    if mode == \"gpt-4o\":\n",
    "        result_file_name = f\"llm/outputs/gpt-4o/{dataset_name}.jsonl\"\n",
    "    elif mode == \"cot\":\n",
    "        result_file_name = f\"llm/outputs/cot_full/{dataset_name}.jsonl\"\n",
    "    else:\n",
    "        result_file_name = f\"llm/outputs/full/{dataset_name}.jsonl\"\n",
    "        # result_file_name = f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "\n",
    "\n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parsing the JSON string into a dict and appending to the list of results\n",
    "            json_object = json.loads(line.strip())\n",
    "            results.append(json_object[\"response\"]['body'][\"choices\"][0]['message']['content'])\n",
    "    \n",
    "    labels = []\n",
    "    explainations = []\n",
    "    \n",
    "    for result in results:\n",
    "        label, explanation = parse_label_explanation(result)\n",
    "        if type(label) == str:\n",
    "            labels += [label.strip()]\n",
    "        else:\n",
    "            labels += [label]\n",
    "\n",
    "        explainations += [explanation]\n",
    "    \n",
    "    #test = pd.read_parquet(os.path.join(\"doccano\", \"random\", \"parquet\", f\"{dataset_name}.pkl\"))\n",
    "    test = pd.read_parquet(os.path.join(\"data\", \"llm_green_nlp_tasks\", f\"{dataset_name}.pkl\"))\n",
    "\n",
    "    test['gpt-4o-mini_label'] = labels\n",
    "    test['gpt-4o-mini_explanation'] = explainations\n",
    "    \n",
    "    if dataset_name in label_readable_mapping:\n",
    "        label2id = {v.lower(): k for k, v in label_readable_mapping[dataset_name]['labels'].items()}\n",
    "\n",
    "        if dataset_name == 'contrarian_claims':\n",
    "            # Split the keys of label2id based on \",\" and create a new mapping\n",
    "            new_label2id = {}\n",
    "            for key, value in label2id.items():\n",
    "                key_parts = [part.strip().lower() for part in key.split(',')]\n",
    "                new_label2id[key_parts[-1]] = value\n",
    "\n",
    "            # Function to map and replace labels in the dataframe\n",
    "            def map_labels(label):\n",
    "                parts = [part.strip().lower() for part in label.split(',')]\n",
    "                return parts[-1]\n",
    "                \n",
    "            test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].apply(map_labels)\n",
    "            test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].str.lower().map(new_label2id)\n",
    "        else:\n",
    "            test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].str.lower().map(label2id)\n",
    "        \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484032b30886601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T09:44:49.322995Z",
     "start_time": "2024-09-17T09:44:49.220889400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_raw_outputs(dataset_name):\n",
    "        \n",
    "    # Loading data from saved file\n",
    "    results = []\n",
    "    result_file_name = f\"llm/outputs/full/{dataset_name}.jsonl\"\n",
    "    \n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parsing the JSON string into a dict and appending to the list of results\n",
    "            json_object = json.loads(line.strip())\n",
    "            results.append(json_object[\"response\"]['body'][\"choices\"][0]['message']['content'])\n",
    "    \n",
    "    labels = []\n",
    "    explainations = []\n",
    "    \n",
    "    for result in results:\n",
    "        label, explanation = parse_label_explanation(result)\n",
    "        labels += [label]\n",
    "        explainations += [explanation]\n",
    "    \n",
    "    test = pd.read_parquet(os.path.join(\"data\", \"llm_green_nlp_tasks\", f\"{dataset_name}.pkl\"))\n",
    "    \n",
    "    test['gpt-4o-mini_label'] = labels\n",
    "    test['gpt-4o-mini_explanation'] = explainations\n",
    "    \n",
    "    if dataset_name in label_readable_mapping:\n",
    "        label2id = {v.lower(): k for k, v in label_readable_mapping[dataset_name]['labels'].items()}\n",
    "\n",
    "        if dataset_name == 'contrarian_claims':\n",
    "            # Split the keys of label2id based on \",\" and create a new mapping\n",
    "            new_label2id = {}\n",
    "            for key, value in label2id.items():\n",
    "                key_parts = [part.strip().lower() for part in key.split(',')]\n",
    "                new_label2id[key_parts[-1]] = value\n",
    "\n",
    "            # Function to map and replace labels in the dataframe\n",
    "            def map_labels(label):\n",
    "                parts = [part.strip().lower() for part in label.split(',')]\n",
    "                return parts[-1]\n",
    "                \n",
    "            test['unprocessed_gpt-4o-mini_label'] = test['gpt-4o-mini_label'].copy()\n",
    "            test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].apply(map_labels)\n",
    "            test['parsing'] = test['gpt-4o-mini_label'].str.lower().map(new_label2id)\n",
    "        else:\n",
    "            test['parsing'] = test['gpt-4o-mini_label'].str.lower().map(label2id)\n",
    "            \n",
    "        test['label_text'] = test['label'].map(label_readable_mapping[dataset_name]['labels'])\n",
    "\n",
    "    return test, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9cd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def create_jsonl_errors_file(test, dataset_name, negative_label, positive_label, n_sample=10):\n",
    "    # False Negatives:\n",
    "    if len(test[(test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)) & (test['gpt-4o-mini_label'].astype(str) == negative_label)]) < 10:\n",
    "        false_negative = test[(test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)) & (test['gpt-4o-mini_label'].astype(str) == negative_label)]\n",
    "    else:\n",
    "        false_negative = test[(test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)) & (test['gpt-4o-mini_label'].astype(str) == negative_label)].sample(n_sample, random_state=42)\n",
    "\n",
    "    false_negative = false_negative.copy()\n",
    "    false_negative['comment'] = \"\"\n",
    "\n",
    "    # False Positives:\n",
    "    if len(test[(test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)) & (test['gpt-4o-mini_label'].astype(str) == positive_label)]) < 10:\n",
    "        false_positive = test[(test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)) & (test['gpt-4o-mini_label'].astype(str) == positive_label)]\n",
    "    else:\n",
    "        false_positive = test[(test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)) & (test['gpt-4o-mini_label'].astype(str) == positive_label)].sample(n_sample, random_state=42)\n",
    "\n",
    "    false_positive = false_positive.copy()\n",
    "    false_positive['comment'] = \"\"\n",
    "\n",
    "    # Add readable labels:\n",
    "    if dataset_name in label_readable_mapping.keys():\n",
    "        false_negative['label'] = false_negative['label'].astype(str).map(label_readable_mapping[dataset_name]['labels'])\n",
    "        false_positive['label'] = false_positive['label'].astype(str).map(label_readable_mapping[dataset_name]['labels'])\n",
    "\n",
    "    # Convert DataFrames to list of dictionaries\n",
    "    false_negative_list = false_negative.to_dict(orient='records')\n",
    "    false_positive_list = false_positive.to_dict(orient='records')\n",
    "\n",
    "    # Define file paths\n",
    "    false_negative_path = f'error_analysis/{dataset_name}_fn.json'\n",
    "    false_positive_path = f'error_analysis/{dataset_name}_fp.json'\n",
    "\n",
    "    # Function to save JSON file if it doesn't exist\n",
    "    def save_pretty_json(data, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "            print(f\"File '{file_path}' saved successfully.\")\n",
    "        else:\n",
    "            print(f\"File '{file_path}' already exists. Skipping save.\")\n",
    "\n",
    "    # Save pretty-printed JSON files if they do not exist\n",
    "    save_pretty_json(false_negative_list, false_negative_path)\n",
    "    save_pretty_json(false_positive_list, false_positive_path)\n",
    "\n",
    "def get_statistics_erros(dataset_name):\n",
    "\n",
    "    false_negative_path = f'error_analysis/{dataset_name}_fn.json'\n",
    "    false_positive_path = f'error_analysis/{dataset_name}_fp.json'\n",
    "\n",
    "    # Function to read and print JSON file\n",
    "    def read_json(file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"Contents of '{file_path}':\")\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "    # Read and print false negatives\n",
    "    false_negatives = read_json(false_negative_path)\n",
    "\n",
    "    # Read and print false positives\n",
    "    false_positive = read_json(false_positive_path)\n",
    "\n",
    "    # False negatives\n",
    "    print(\"False Negatives:\")\n",
    "    if len(false_negatives) > 0:\n",
    "        labels = [ex['comment'].split(\":\")[0].split(\",\") for ex in false_negatives]\n",
    "        flattened_labels = set([label.strip() for sublist in labels for label in sublist])\n",
    "\n",
    "        for label in flattened_labels:\n",
    "            count_label = 0\n",
    "            for _labels in labels:\n",
    "                if label in _labels:\n",
    "                    count_label += 1\n",
    "            print(label, count_label, len(labels), count_label/len(labels))\n",
    "\n",
    "    # False positives\n",
    "    print(\"False Positives:\")\n",
    "    if len(false_positive) > 0:\n",
    "        labels = [ex['comment'].split(\":\")[0].split(\",\") for ex in false_positive]\n",
    "        flattened_labels = set([label.strip() for sublist in labels for label in sublist])\n",
    "\n",
    "        for label in flattened_labels:\n",
    "            count_label = 0\n",
    "            for _labels in labels:\n",
    "                if label in _labels:\n",
    "                    count_label += 1\n",
    "            print(label, count_label, len(labels), count_label/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def create_jsonl_errors_file_multilabel(test, dataset_name, n_sample=10, use_readable_labels=True):\n",
    "\n",
    "    if 'Date' in test.columns:\n",
    "        test = test.drop(columns=['Date'])\n",
    "    \n",
    "    # False Negatives:\n",
    "    if len(test[(test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str))]) < 10:\n",
    "        false_negative = test[(test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str))]\n",
    "    else:\n",
    "        false_negative = test[(test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str))].sample(n_sample, random_state=42)\n",
    "\n",
    "    false_negative = false_negative.copy()\n",
    "    false_negative['comment'] = \"\"\n",
    "\n",
    "    # Add readable labels:\n",
    "    if (dataset_name in label_readable_mapping.keys()) and (use_readable_labels):\n",
    "        false_negative['label'] = false_negative['label'].astype(str).map(label_readable_mapping[dataset_name]['labels'])\n",
    "\n",
    "    # Convert DataFrames to list of dictionaries\n",
    "    false_negative_list = false_negative.to_dict(orient='records')\n",
    "\n",
    "    # Define file paths\n",
    "    false_negative_path = f'error_analysis/{dataset_name}_fn.json'\n",
    "\n",
    "    # Function to save JSON file if it doesn't exist\n",
    "    def save_pretty_json(data, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "            print(f\"File '{file_path}' saved successfully.\")\n",
    "        else:\n",
    "            print(f\"File '{file_path}' already exists. Skipping save.\")\n",
    "\n",
    "    # Save pretty-printed JSON files if they do not exist\n",
    "    save_pretty_json(false_negative_list, false_negative_path)\n",
    "\n",
    "def get_statistics_errors_multi(dataset_name):\n",
    "\n",
    "    false_negative_path = f'error_analysis/{dataset_name}_fn.json'\n",
    "\n",
    "    # Function to read and print JSON file\n",
    "    def read_json(file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"Contents of '{file_path}':\")\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "    # Read and print false negatives\n",
    "    false_negatives = read_json(false_negative_path)\n",
    "\n",
    "    # False negatives\n",
    "    print(\"False Negatives:\")\n",
    "    if len(false_negatives) > 0:\n",
    "        labels = [ex['comment'].split(\":\")[0].split(\",\") for ex in false_negatives]\n",
    "        flattened_labels = set([label.strip() for sublist in labels for label in sublist])\n",
    "\n",
    "        for label in flattened_labels:\n",
    "            count_label = 0\n",
    "            for _labels in labels:\n",
    "                if label in _labels:\n",
    "                    count_label += 1\n",
    "            print(label, count_label, len(labels), count_label/len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e9e64e83f8d49",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## ClimateFEVER_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b773dc3ab78df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T11:08:55.404496500Z",
     "start_time": "2024-09-17T11:08:55.290714600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"climateFEVER_claim\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ea1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test['label']\n",
    "predicted_labels = test['gpt-4o-mini_label']\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)))\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3069b4",
   "metadata": {},
   "source": [
    "## ClimateFEVER evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climateFEVER_evidence\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test['label']\n",
    "predicted_labels = test['gpt-4o-mini_label']\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize=\"true\"),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test, dataset_name=dataset_name)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1049e4554e2b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## SciDCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b3ec46fa8e2b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T19:27:34.832914100Z",
     "start_time": "2024-09-13T19:27:34.747380600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"sciDCC\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test['label']\n",
    "predicted_labels = test['gpt-4o-mini_label']\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize=\"true\"),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    # Plot both confusion matrices side by side using subplots\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Assuming 'generator' and 'dataset_name' are defined elsewhere in your code\n",
    "train, test, dev = generator.load_dataset_unprocessed(dataset_name)\n",
    "\n",
    "timeline_data = pd.concat([train, test, dev], ignore_index=True)\n",
    "timeline_data['year'] = pd.to_datetime(timeline_data['Date']).dt.year\n",
    "\n",
    "plot_data = timeline_data.groupby(['year', 'label']).size().unstack(fill_value=0)\n",
    "label_proportions = plot_data.div(plot_data.sum(axis=1), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Use a colormap for better color distinction\n",
    "cmap = cm.get_cmap(\"mako\", len(label_proportions.columns))\n",
    "colors = [cmap(i) for i in range(len(label_proportions.columns))]\n",
    "\n",
    "# Plot stacked area chart\n",
    "label_proportions.plot(kind='area', stacked=True, ax=ax, linewidth=0, alpha=0.8, color=colors)\n",
    "\n",
    "# Improve text annotations\n",
    "cumulative = label_proportions.cumsum(axis=1)\n",
    "\n",
    "for i, column in enumerate(label_proportions.columns):\n",
    "    max_idx = label_proportions[column].idxmax()\n",
    "    max_value = (cumulative[column] - label_proportions[column] / 2).loc[max_idx]\n",
    "    \n",
    "    text = ax.text(\n",
    "        x=max_idx, y=max_value, s=column, \n",
    "        verticalalignment='center', horizontalalignment='center', \n",
    "        fontsize=11, color=\"white\", fontweight=\"bold\"\n",
    "    )\n",
    "    text.set_path_effects([PathEffects.withStroke(linewidth=4, foreground='black')])  # Add contrast\n",
    "\n",
    "# Remove legend\n",
    "ax.legend().set_visible(False)\n",
    "\n",
    "# Enhance grid and remove unnecessary borders\n",
    "ax.grid(axis=\"y\", linestyle=\"dashed\", alpha=0.5)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Improve titles and labels\n",
    "plt.title('Evolution of Labels Over Time', fontsize=16, fontweight=\"bold\", pad=20)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Proportion of Labels', fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=45, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab46b52b99a9852",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The errors might be explained by the evolution of labels overtime as describ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc057e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test, dataset_name=dataset_name)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8dd43647e7dc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## contrarian_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963624d30993b65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:31:33.630086700Z",
     "start_time": "2024-09-13T10:31:33.581082200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"contrarian_claims\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test['label'] = test['label'].map(label_readable_mapping[dataset_name]['labels'])\n",
    "test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].map(label_readable_mapping[dataset_name]['labels'])\n",
    "\n",
    "# test[test['label'].astype(str) != test['gpt-4o-mini_label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8078315",
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_label_mapping = {\n",
    "    'CO2 is beneficial/not a pollutant': 'CO2 beneficial',\n",
    "    'Climate sensitivity is low/negative feedbacks reduce warming': 'Low sensitivity',\n",
    "    'Species/plants/reefs aren’t showing climate impacts/are benefiting from climate change': 'Species benefiting',\n",
    "    'Climate movement is unreliable/alarmist/corrupt': 'Climate alarmist',\n",
    "    'methods & models)': 'Science uncertain',\n",
    "    'Clean energy technology/biofuels won’t work': 'Clean tech won’t work',\n",
    "    'Climate policies (mitigation or adaptation) are harmful': 'Policies harmful',\n",
    "    'Climate policies are ineffective/flawed': 'Policies ineffective',\n",
    "    'People need energy (e.g. from fossil fuels/nuclear)': 'Energy demand',\n",
    "    'Climate hasn’t warmed/changed over the last (few) decade(s)': 'No warming',\n",
    "    'Extreme weather isn’t increasing/has happened before/isn’t linked to climate change': 'No extreme link',\n",
    "    'Ice/permafrost/snow cover isn’t melting': 'Ice stable',\n",
    "    'Sea level rise is exaggerated/not accelerating': 'Sea level stable',\n",
    "    'Weather is cold/snowing': 'Cold weather',\n",
    "    'We’re heading into an ice age/global cooling': 'Global cooling',\n",
    "    'It’s natural cycles/variation': 'Natural cycles',\n",
    "    'There’s no evidence for greenhouse effect/carbon dioxide driving climate change': 'No greenhouse effect',\n",
    "    'No claim': 'No claim'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e942884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_ranges(categories):\n",
    "    category_ranges = {}\n",
    "    current_category = categories[0]\n",
    "    start_idx = 0\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        if category != current_category:\n",
    "            category_ranges[current_category] = (start_idx, i - 1)\n",
    "            current_category = category\n",
    "            start_idx = i\n",
    "\n",
    "    category_ranges[current_category] = (start_idx, len(categories) - 1)\n",
    "    return category_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b871504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib.patheffects import withStroke\n",
    "from matplotlib import patches\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test['label'].astype(str)\n",
    "predicted_labels = test['gpt-4o-mini_label'].astype(str)\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize=\"true\")\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "color_category = [c.split(',')[0] for c in class_labels]  # Get the category part of each label\n",
    "short_labels = [shorter_label_mapping[c.split(',')[-1].strip()] for c in class_labels]  # Shorten label for display\n",
    "\n",
    "# Build label info: list of tuples (index, category, label)\n",
    "label_info = [(i, color_category[i], short_labels[i]) for i in range(len(class_labels))]\n",
    "\n",
    "# Sort label_info by category to group labels\n",
    "label_info_sorted = sorted(label_info, key=lambda x: x[1])\n",
    "\n",
    "# Get new order of indices\n",
    "new_order = [x[0] for x in label_info_sorted]\n",
    "rearranged_labels = [x[2] for x in label_info_sorted]\n",
    "rearranged_categories = [x[1] for x in label_info_sorted]\n",
    "\n",
    "# Rearrange the confusion matrix according to the new label order\n",
    "cm_rearranged = cm[np.ix_(new_order, new_order)]\n",
    "\n",
    "# Generate a high-contrast palette using seaborn\n",
    "palette = sns.color_palette(\"tab10\", len(set(rearranged_categories)))  # Use \"hsv\" palette for distinct colors\n",
    "category_to_color = {\n",
    "    category: palette[i] for i, category in enumerate(sorted(set(rearranged_categories)))\n",
    "}\n",
    "\n",
    "# Get the ranges for each category\n",
    "category_ranges = get_category_ranges(rearranged_categories)\n",
    "\n",
    "# Display the confusion matrix with colored labels and distinct colors for zero values\n",
    "def plot_confusion_matrix(cm, labels, categories, cell_width=0.7, cell_height=0.7):\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    cm = np.round(cm,2)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "\n",
    "    # Create a uniform background for all cells\n",
    "    ax.matshow(np.zeros_like(cm), cmap=\"gray\", alpha=0.1)  # Light gray background\n",
    "\n",
    "\n",
    "    # Color each cell of the matrix according to its value and category\n",
    "    for (i, j), val in np.ndenumerate(cm):\n",
    "        # Color zero-value cells with light gray\n",
    "        if val == 0:\n",
    "            color = (0.95, 0.95, 0.95)  # Light gray for zeros\n",
    "        else:\n",
    "            # Color non-zero cells based on their category\n",
    "            category_i = categories[i]  # Get category for true label\n",
    "            category_j = categories[j]  # Get category for predicted label\n",
    "            if category_i == category_j:\n",
    "                color = category_to_color[category_i]\n",
    "            else:\n",
    "                color = (0.8, 0.8, 0.8)  # Light color for mismatched categories\n",
    "\n",
    "        # Create a filled rectangle for each cell with custom color\n",
    "        rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=True, color=color, alpha=0.2, edgecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Plot confusion matrix values\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    # plt.title(\"Confusion Matrix with Highlighted Zero and Non-Zero Values\")\n",
    "\n",
    "    # # Set the color of x-axis tick labels based on categories\n",
    "    # for label in ax.get_xticklabels():\n",
    "    #     idx = labels.index(label.get_text())\n",
    "    #     category = categories[idx]\n",
    "    #     # label.set_color(category_to_color[category])\n",
    "    #     label.set_path_effects([withStroke(linewidth=2, foreground=category_to_color[category])])\n",
    "\n",
    "\n",
    "    # # Set the color of y-axis tick labels based on categories\n",
    "    # for label in ax.get_yticklabels():\n",
    "    #     idx = labels.index(label.get_text())\n",
    "    #     category = categories[idx]\n",
    "    #     label.set_color(category_to_color[category])\n",
    "    #     label.set_path_effects([withStroke(linewidth=2, foreground=\"black\")])\n",
    "        # Add rectangles around groups to highlight categories\n",
    "    for category, (start, end) in category_ranges.items():\n",
    "        rect = patches.Rectangle(\n",
    "            (start - 0.5, start - 0.5),\n",
    "            end - start + 1,\n",
    "            end - start + 1,\n",
    "            linewidth=2,\n",
    "            edgecolor=category_to_color[category],\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Add legend for categories\n",
    "    handles = [plt.Line2D([0], [0], color=category_to_color[cat], lw=4) for cat in sorted(set(categories))]\n",
    "\n",
    "    plt.legend(handles, sorted(set(categories)), title='Categories', bbox_to_anchor=(0.5, -0.03), loc='upper center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm_rearranged, rearranged_labels, rearranged_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c166d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test, dataset_name=dataset_name)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa9930cfe5c874",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## ClimaINS_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9eee244213439",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T16:03:15.984854500Z",
     "start_time": "2024-09-16T16:03:15.855399600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"ClimaINS_ours\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test['label'] = test['label'].astype(str).map(label_readable_mapping[dataset_name]['labels'])\n",
    "test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].astype(str).map(label_readable_mapping[dataset_name]['labels'])\n",
    "\n",
    "test[test['label'].astype(str) != test['gpt-4o-mini_label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d73ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test['label']\n",
    "predicted_labels = test['gpt-4o-mini_label']\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize=\"true\"),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab479f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test, dataset_name=dataset_name, use_readable_labels=False)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b370fe80e94ea79",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## climaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb279af60ae49b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:52:34.795932900Z",
     "start_time": "2024-09-16T12:52:34.708926600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"climaQA\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "# test['label'] = test['label'].astype(str).map(label_readable_mapping[dataset_name]['labels'])\n",
    "# test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].astype(str).map(label_readable_mapping[dataset_name]['labels'])\n",
    "\n",
    "test[test['label'].astype(str) != test['gpt-4o-mini_label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare to collect accuracy data\n",
    "accuracy_data = []\n",
    "\n",
    "# Loop over each query type in the 'query' column\n",
    "for q in test['query'].value_counts().index:\n",
    "    accuracy = classification_report(\n",
    "        y_pred=test[test['query'] == q]['gpt-4o-mini_label'].astype(str),\n",
    "        y_true=test[test['query'] == q]['label'].astype(str),\n",
    "        zero_division=0.0,\n",
    "        output_dict=True\n",
    "    )['macro avg']['f1-score']\n",
    "    try:\n",
    "        precision = classification_report(\n",
    "            y_pred=test[test['query'] == q]['gpt-4o-mini_label'].astype(str),\n",
    "            y_true=test[test['query'] == q]['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "            output_dict=True\n",
    "        )['1']['precision']\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = classification_report(\n",
    "            y_pred=test[test['query'] == q]['gpt-4o-mini_label'].astype(str),\n",
    "            y_true=test[test['query'] == q]['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "            output_dict=True\n",
    "        )['1']['recall']\n",
    "    except:\n",
    "        recall = 0\n",
    "    size = len(test[test['query'] == q])\n",
    "    \n",
    "    # Append the query and corresponding accuracy to the list\n",
    "    accuracy_data.append((q, accuracy, size, precision, recall))\n",
    "\n",
    "# Convert the list to a DataFrame for plotting\n",
    "accuracy_df = pd.DataFrame(accuracy_data, columns=['query', 'accuracy', 'size', \"precision\", \"recall\"])\n",
    "\n",
    "truncate_label = dict()\n",
    "for i, q in enumerate(accuracy_df['query'].unique()):\n",
    "    truncate_label[q] = f\"Q{i}\"\n",
    "\n",
    "# Apply the truncation function to the query labels\n",
    "accuracy_df['query'] = accuracy_df['query'].map(truncate_label)\n",
    "\n",
    "\n",
    "# Plotting the accuracy and size on dual y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# First plot for accuracy\n",
    "ax1.bar(accuracy_df['query'], accuracy_df['accuracy'], color='C0', label='Accuracy')\n",
    "ax1.set_xlabel('Question')\n",
    "ax1.set_ylabel('macro F1-score', color='C0')\n",
    "ax1.tick_params(axis='y', labelcolor='C0')\n",
    "ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "ax1.set_ylim(0, 1)  # Set y-axis limits from 0 to 1 for clarity\n",
    "\n",
    "# Second plot for size of the dataset, sharing the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(accuracy_df['query'], accuracy_df['size'], color='C1', marker='o', label='Size')\n",
    "ax2.set_ylabel('Size of Dataset', color='C1')\n",
    "ax2.tick_params(axis='y', labelcolor='C1')\n",
    "\n",
    "# Set title and adjust layout\n",
    "plt.title('macro F1-score and Size for Each Question')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e945106",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"ClimaQA\"\n",
    "negative_label = \"0\"\n",
    "positive_label = \"1\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, negative_label=negative_label, positive_label=positive_label, dataset_name=dataset_name)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afb088cf3db263",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## logicClimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723cffe643cd04ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:21:24.022106600Z",
     "start_time": "2024-09-16T18:21:23.844455300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "dataset_name = \"logicClimate\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert string labels to lists\n",
    "y_true = test['label'].apply(literal_eval)\n",
    "y_pred = test['gpt-4o-mini_label'].apply(lambda x: x.split(\",\"))\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the binarizer and transform the labels\n",
    "y_true_binarized = mlb.fit_transform(y_true)\n",
    "y_pred_binarized = mlb.transform(y_pred)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=y_pred_binarized, \n",
    "            y_true=y_true_binarized,\n",
    "            target_names=mlb.classes_,\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdbf0b0",
   "metadata": {},
   "source": [
    "## DESMOG /GW Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71464466",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "test_cot['label'] = test_cot['label'].astype(str)\n",
    "test_cot['gpt-4o-mini_label'] = test_cot['gpt-4o-mini_label'].astype(str)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test_cot['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test_cot[test_cot['gpt-4o-mini_label'].astype(str) != test_cot['label'].astype(str)].copy()\n",
    "errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "merged_errors = errors.merge(errors_cot, on=['text', 'label'], how=\"outer\", suffixes=(\"_zero\", \"_cot\"))\n",
    "\n",
    "print(len(merged_errors[merged_errors['gpt-4o-mini_label_zero'].isna()]), len(merged_errors[merged_errors['gpt-4o-mini_label_cot'].isna()]), len(merged_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test_cot['label']\n",
    "predicted_labels = test_cot['gpt-4o-mini_label']\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize=\"true\"),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test_cot, dataset_name=dataset_name, use_readable_labels=False)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4783e5c9",
   "metadata": {},
   "source": [
    "## ClimateStance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ac15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "test_cot['label'] = test_cot['label'].astype(str)\n",
    "test_cot['label'] = test_cot['label'].map(label_readable_mapping['climateStance']['labels'])\n",
    "test_cot['gpt-4o-mini_label'] = test_cot['gpt-4o-mini_label'].astype(str)\n",
    "test_cot['gpt-4o-mini_label'] = test_cot['gpt-4o-mini_label'].map(label_readable_mapping['climateStance']['labels'])\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test_cot['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test_cot[test_cot['gpt-4o-mini_label'].astype(str) != test_cot['label'].astype(str)].copy()\n",
    "errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "merged_errors = errors.merge(errors_cot, on=['text', 'label'], how=\"outer\", suffixes=(\"_zero\", \"_cot\"))\n",
    "\n",
    "print(len(merged_errors[merged_errors['gpt-4o-mini_label_zero'].isna()]), len(merged_errors[merged_errors['gpt-4o-mini_label_cot'].isna()]), len(merged_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa59d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test_cot['label']\n",
    "predicted_labels = test_cot['gpt-4o-mini_label']\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize=\"true\"),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test_cot, dataset_name=dataset_name, use_readable_labels=False)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608cfc12c55d92e8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Lobbymap query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551477f8566a23e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:32:33.020589300Z",
     "start_time": "2024-09-17T12:32:32.955412800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dataset_name = \"lobbymap_query_origin\"\n",
    "\n",
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "\n",
    "test_cot = test_cot[test_cot[\"query\"].astype(str) != \"[None]\"].copy()\n",
    "\n",
    "# Convert string labels to lists\n",
    "y_true = test_cot['query'].apply(lambda x: [map_lobbymap_stance[e] for e in x])\n",
    "y_pred = test_cot['gpt-4o-mini_label'].apply(lambda x: [e.strip() for e in x.split(\",\")])\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the binarizer and transform the labels\n",
    "y_true_binarized = mlb.fit_transform(y_true)\n",
    "y_pred_binarized = mlb.transform(y_pred)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_pred=y_pred_binarized,\n",
    "        y_true=y_true_binarized,\n",
    "        target_names=mlb.classes_,\n",
    "        zero_division=0.0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4d616151970f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:33:00.744952400Z",
     "start_time": "2024-09-17T12:33:00.678886Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Loading data from saved file\n",
    "results = []\n",
    "result_file_name = f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "\n",
    "with open(result_file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        # Parsing the JSON string into a dict and appending to the list of results\n",
    "        json_object = json.loads(line.strip())\n",
    "        results.append(json_object[\"response\"]['body'][\"choices\"][0]['message']['content'])\n",
    "\n",
    "labels = []\n",
    "explainations = []\n",
    "\n",
    "for result in results:\n",
    "    label, explanation = parse_label_explanation(result)\n",
    "    labels += [label]\n",
    "    explainations += [explanation]\n",
    "\n",
    "test = pd.read_parquet(os.path.join(\"doccano\", \"random\", \"parquet\", f\"{dataset_name}.pkl\"))\n",
    "\n",
    "test['gpt-4o-mini_label'] = labels\n",
    "test['gpt-4o-mini_explanation'] = explainations\n",
    "\n",
    "if dataset_name in label_readable_mapping:\n",
    "    label2id = {v.lower(): k for k, v in label_readable_mapping[dataset_name]['labels'].items()}\n",
    "    test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].str.lower().map(label2id)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8945c30b",
   "metadata": {},
   "source": [
    "## Lobbymap Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lobbymap_pages\"\n",
    "\n",
    "test = find_errors(dataset_name, mode=\"cot\")\n",
    "\n",
    "test['label'] = 1 * test['label']\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f653ebd9077d994",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Lobbymap Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fc1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "relieve_stance = {\n",
    "        'strongly_supporting': 'supporting',\n",
    "        'supporting': 'supporting',\n",
    "        'no_position_or_mixed_position': 'no_position',\n",
    "        'not_supporting': 'not_supporting',\n",
    "        'opposing': 'not_supporting',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lobbymap_stance_origin\"\n",
    "\n",
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "\n",
    "test_cot = test_cot[~test_cot['query'].isna()].copy()\n",
    "\n",
    "test_cot['label'] = test_cot['stance'].copy()\n",
    "test_cot['label'] = test_cot['label'].astype(str)\n",
    "# test['label'] = test['label'].map(relieve_stance)\n",
    "test_cot['gpt-4o-mini_label'] = test_cot['gpt-4o-mini_label'].astype(str)\n",
    "# test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].map(relieve_stance)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'], \n",
    "            y_true=test_cot['label'],\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababffabb4a5e7ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:41:33.195208700Z",
     "start_time": "2024-09-16T18:41:33.114932900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_cot['relieved_label'] = test_cot['label'].map(relieve_stance)\n",
    "test_cot['relieved_gpt-4o-mini_label'] = test_cot['gpt-4o-mini_label'].map(relieve_stance)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['relieved_gpt-4o-mini_label'], \n",
    "            y_true=test_cot['relieved_label'],\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b14219",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cot['comment'] = \"\"\n",
    "test_cot[test_cot['label'] != test_cot['gpt-4o-mini_label']].sample(10).to_json('paper_utils\\\\error_analysis\\\\lobbymap_query_stance_fn.json', orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8441dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_statistics_errors_multi(dataset_name=\"lobbymap_query_stance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71042b7351c7eb92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T19:44:01.755377Z",
     "start_time": "2024-09-16T19:44:01.597348800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_page_idx(l):\n",
    "    list_of_lists = [e['page_indices'] for e in l]\n",
    "    flattened_list = [item for sublist in list_of_lists for item in sublist]\n",
    "    return list(set(flattened_list))\n",
    "\n",
    "def get_page_stance_map(ds):\n",
    "    ds_exploded = ds.explode('evidences')\n",
    "    ds_exploded['page_indices'] = ds_exploded['evidences'].apply(lambda x: x['page_indices'])\n",
    "    ds_exploded['comment'] = ds_exploded['evidences'].apply(lambda x: x['comment'])\n",
    "    ds_exploded['query'] = ds_exploded['evidences'].apply(lambda x: x['query'])\n",
    "    ds_exploded['stance'] = ds_exploded['evidences'].apply(lambda x: x['stance'])\n",
    "    mapping = ds_exploded[['document_id', 'page_indices', 'query', 'stance', 'comment']].explode('page_indices')\n",
    "    return mapping.reset_index()\n",
    "\n",
    "def reconstruct_page(dataset_df):\n",
    "    exploded_train = dataset_df[['document_id', 'sentences']].explode('sentences')\n",
    "\n",
    "    exploded_train['page_idx'] = exploded_train['sentences'].apply(lambda x: x['page_idx'])\n",
    "    exploded_train['sentence_id'] = exploded_train['sentences'].apply(lambda x: x['sentence_id'])\n",
    "    exploded_train['block_idx'] = exploded_train['sentences'].apply(lambda x: x['block_idx'])\n",
    "    exploded_train['text'] = exploded_train['sentences'].apply(lambda x: x['text'])\n",
    "\n",
    "    page_inputs = exploded_train.groupby(by=['document_id', 'page_idx', 'block_idx'])[\n",
    "        'text'].sum().reset_index()\n",
    "    page_inputs = page_inputs.groupby(by=['document_id', 'page_idx'])['text'].apply(lambda x: \"\\\\n\".join(x))\n",
    "\n",
    "    return page_inputs.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b9d8936aa6ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:38:12.235706500Z",
     "start_time": "2024-09-17T12:37:57.471016900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(), \"data\", \"lobbymap\", \"lobbymap_dataset\")\n",
    "\n",
    "train_path = os.path.join(folder_path, \"train.jsonl\")\n",
    "raw_train = pd.read_json(train_path, lines=True)\n",
    "\n",
    "test_path = os.path.join(folder_path, \"test.jsonl\")\n",
    "raw_test = pd.read_json(test_path, lines=True)\n",
    "\n",
    "dev_path = os.path.join(folder_path, \"valid.jsonl\")\n",
    "raw_dev = pd.read_json(dev_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f772db58f0a3ecc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T21:10:06.623620500Z",
     "start_time": "2024-09-16T21:09:51.969395200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def binary_stance_dataset(raw):\n",
    "    map_stance_label = get_page_stance_map(raw)\n",
    "    pages = reconstruct_page(raw.copy())\n",
    "    stance = map_stance_label.merge(pages, left_on=['document_id', 'page_indices'],\n",
    "                                  right_on=['document_id', 'page_idx'], how='left')\n",
    "    stance = stance[['text', 'query', 'stance', 'comment', \"document_id\"]].copy()\n",
    "    stance.columns=['text', 'query', 'label', 'comment', \"document_id\"]\n",
    "    return stance\n",
    "\n",
    "x_train = binary_stance_dataset(raw_train)\n",
    "x_test = binary_stance_dataset(raw_test)\n",
    "x_dev = binary_stance_dataset(raw_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf350ab2397025a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:52:40.806481600Z",
     "start_time": "2024-09-17T12:52:40.754039500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def reconstruct_pages(sentences):\n",
    "    # Group sentences by page index\n",
    "    pages = defaultdict(list)\n",
    "    for sentence in sentences:\n",
    "        page_idx = sentence['page_idx']\n",
    "        pages[page_idx].append(sentence)\n",
    "    \n",
    "    # Reconstruct pages\n",
    "    reconstructed_pages = {}\n",
    "    for page_idx, page_sentences in pages.items():\n",
    "        # Sort sentences by block index and block sentence index\n",
    "        sorted_sentences = sorted(\n",
    "            page_sentences,\n",
    "            key=lambda x: (x['block_idx'], x['block_sentence_idx'])\n",
    "        )\n",
    "        # Concatenate the text of the sentences\n",
    "        page_text = ' '.join(sentence['text'] for sentence in sorted_sentences)\n",
    "        reconstructed_pages[page_idx] = page_text\n",
    "    return reconstructed_pages\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "sampled_documents['reconstructed_pages'] = sampled_documents['sentences'].apply(reconstruct_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835fe12c9cea20f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:01:08.002721300Z",
     "start_time": "2024-09-17T13:01:07.918350Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "X_test = []\n",
    "for i, r in sampled_documents.iterrows():\n",
    "    for evidence in r[\"evidences\"]:\n",
    "        X_test += [r['reconstructed_pages']]\n",
    "        y_true += [(evidence['query'], evidence['stance'], evidence['page_indices'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96824d7de463f1c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:04:10.018042900Z",
     "start_time": "2024-09-17T13:04:09.871392800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"X\":X_test,\n",
    "    \"y\":y_true\n",
    "}).to_csv('llm/dataset/lobbymap.csv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f16b9e30be5a91",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Lobbymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lobbymap_query_origin\"\n",
    "\n",
    "test_query = find_errors(dataset_name, mode=\"cot\")\n",
    "\n",
    "dataset_name = \"lobbymap_pages\"\n",
    "\n",
    "test_page = find_errors(dataset_name, mode=\"cot\")\n",
    "test_page = test_page.drop_duplicates(subset=['document_id', 'page_id'], keep=\"first\")\n",
    "\n",
    "query_page = test_query.merge(test_page, on=['document_id', 'page_id', \"text\"], how=\"left\", suffixes=(\"_query\", \"_page\"))\n",
    "\n",
    "query_page['comment_page'] = \"\"\n",
    "query_page['comment_query'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536d6fb11da1b0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:38:05.612106800Z",
     "start_time": "2024-09-17T13:38:05.495332Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_lobbymap_label(text):\n",
    "    # Regular expression to extract Label and Explanation\n",
    "    label_pattern = r'Evidences:\\s*(.*)'\n",
    "    explanation_pattern = r'Explanation:\\s*(.*)'\n",
    "\n",
    "    # Find the label\n",
    "    label_match = re.search(label_pattern, text)\n",
    "    label = label_match.group(1) if label_match else None\n",
    "\n",
    "    # Find the explanation\n",
    "    explanation_match = re.search(explanation_pattern, text, re.DOTALL)\n",
    "    explanation = explanation_match.group(1).strip() if explanation_match else None\n",
    "    \n",
    "    # label = label.replace('[', \"\").replace(']', \"\")\n",
    "\n",
    "    return label, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bc5e3eae3928b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:39:40.928091Z",
     "start_time": "2024-09-17T13:39:40.848366300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"lobbymap\"\n",
    "        \n",
    "# Loading data from saved file\n",
    "results = []\n",
    "result_file_name = f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "\n",
    "with open(result_file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        # Parsing the JSON string into a dict and appending to the list of results\n",
    "        json_object = json.loads(line.strip())\n",
    "        results.append(json_object[\"response\"]['body'][\"choices\"][0]['message']['content'])\n",
    "\n",
    "labels = []\n",
    "explainations = []\n",
    "\n",
    "for result in results:\n",
    "    label, explanation = parse_lobbymap_label(result)\n",
    "    labels += [label]\n",
    "    explainations += [explanation]\n",
    "\n",
    "test = pd.read_csv('llm/dataset/lobbymap.csv', sep=\"\\t\")\n",
    "test['gpt-4o-mini_label'] = labels\n",
    "test['gpt-4o-mini_explanation'] = explainations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d3024d4d4cc5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:52:41.390268300Z",
     "start_time": "2024-09-17T14:52:41.145963500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "parsed_test = test.groupby('X').agg({\n",
    "    \"y\":list,\n",
    "    \"gpt-4o-mini_label\":\"first\",\n",
    "    \"gpt-4o-mini_explanation\":\"first\"\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218e614c0f903cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:52:42.158979700Z",
     "start_time": "2024-09-17T14:52:42.137289700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Function to ensure correct tuple format\n",
    "def ensure_tuple_format(row):\n",
    "    # Check if row is a list of tuples, if not convert each element to a tuple\n",
    "    if len(row) == 3:\n",
    "        for el in row:\n",
    "            if not isinstance(el, tuple):  # Assuming each tuple should have 3 elements\n",
    "                return [tuple(row)]\n",
    "    return row\n",
    "\n",
    "parsed_test['predicted_evidences'] = parsed_test['gpt-4o-mini_label'].apply(lambda x: ensure_tuple_format(list(literal_eval(x))))\n",
    "parsed_test['gold_evidences'] = parsed_test['y'].apply(lambda x: ensure_tuple_format([literal_eval(el) for el in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf90921a080aca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:56:58.244350200Z",
     "start_time": "2024-09-17T14:56:58.171274800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from src.lobbymap.evaluate_f1 import evaluate_strict_f1, evaluate_overlap_f1, evaluate_document_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3db9aef8eb22ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:53:18.203464900Z",
     "start_time": "2024-09-17T14:53:17.981416900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = parsed_test.copy()\n",
    "df['document_id'] = df.index.astype(str)\n",
    "\n",
    "gold_jds = []\n",
    "pred_jds = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    document_id = row['document_id']\n",
    "    \n",
    "    # Process gold evidences\n",
    "    gold_evidences = []\n",
    "    for evidence in row['gold_evidences']:\n",
    "        query, stance, page_indices = evidence\n",
    "        if not isinstance(page_indices, list):\n",
    "            page_indices = [page_indices]  # Ensure page_indices is a list\n",
    "        gold_evidences.append({\n",
    "            'query': query,\n",
    "            'stance': stance,\n",
    "            'page_indices': page_indices\n",
    "        })\n",
    "    \n",
    "    gold_jds.append({\n",
    "        'document_id': document_id,\n",
    "        'evidences': gold_evidences\n",
    "    })\n",
    "    \n",
    "    # Process predicted evidences\n",
    "    predicted_evidences = []\n",
    "    for evidence in row['predicted_evidences']:\n",
    "        query, stance, page_indices = evidence\n",
    "        if not isinstance(page_indices, list):\n",
    "            page_indices = [page_indices]  # Ensure page_indices is a list\n",
    "        predicted_evidences.append({\n",
    "            'query': query,\n",
    "            'stance': stance,\n",
    "            'page_indices': page_indices\n",
    "        })\n",
    "    \n",
    "    pred_jds.append({\n",
    "        'document_id': document_id,\n",
    "        'evidences': predicted_evidences\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5a12777321b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:54:03.993563300Z",
     "start_time": "2024-09-17T14:54:03.827120600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_strict = evaluate_strict_f1(gold_jds=gold_jds, pred_jds=pred_jds)\n",
    "result_document = evaluate_document_f1(gold_jds=gold_jds, pred_jds=pred_jds)\n",
    "result_overlap = evaluate_overlap_f1(gold_jds=gold_jds, pred_jds=pred_jds)\n",
    "\n",
    "\n",
    "print(\"GPT-4o-mini\", \"&\", result_document['page']['f'], \"&\", result_document['query']['f'], \"&\", result_document['stance']['f'], \"&\", result_overlap['page']['f'], \"&\", result_overlap['query']['f'], \"&\", result_overlap['stance']['f'], \"&\", result_strict['page']['f'], \"&\", result_strict['query']['f'], \"&\", result_strict['stance']['f'], \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514804dff3cc814",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Clima Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test_cot['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test_cot[test_cot['gpt-4o-mini_label'].astype(str) != test_cot['label'].astype(str)].copy()\n",
    "errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "merged_errors = errors.merge(errors_cot, on=['text', 'label'], how=\"outer\", suffixes=(\"_zero\", \"_cot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"0\"\n",
    "positive_label = \"1\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb5095",
   "metadata": {},
   "source": [
    "## Climatext wiki/10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climatext_wiki\"\n",
    "\n",
    "test = find_errors(dataset_name, mode=\"cot\")\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"0\"\n",
    "positive_label = \"1\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climatext_10k\"\n",
    "\n",
    "test = find_errors(dataset_name, mode=\"cot\")\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5961d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"0\"\n",
    "positive_label = \"1\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762759c6",
   "metadata": {},
   "source": [
    "## climate_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test_cot['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test_cot[test_cot['gpt-4o-mini_label'].astype(str) != test_cot['label'].astype(str)].copy()\n",
    "errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "merged_errors = errors.merge(errors_cot, on=['text', 'label'], how=\"outer\", suffixes=(\"_zero\", \"_cot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048cd4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"no\"\n",
    "positive_label = \"yes\"\n",
    "n_sample = 11\n",
    "\n",
    "create_jsonl_errors_file(test, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d5666",
   "metadata": {},
   "source": [
    "## Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f411c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climate_specificity\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53737993",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test, dataset_name=dataset_name, use_readable_labels=False)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed11b5e",
   "metadata": {},
   "source": [
    "## climate tcfd recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81586f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climate_tcfd_recommendations\"\n",
    "\n",
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test_cot['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test_cot[test_cot['gpt-4o-mini_label'].astype(str) != test_cot['label'].astype(str)].copy()\n",
    "errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "merged_errors = errors.merge(errors_cot, on=['text', 'label'], how=\"outer\", suffixes=(\"_zero\", \"_cot\"))\n",
    "\n",
    "print(len(merged_errors[merged_errors['gpt-4o-mini_label_zero'].isna()]), len(merged_errors[merged_errors['gpt-4o-mini_label_cot'].isna()]), len(merged_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test_cot['label'].astype(str)\n",
    "predicted_labels = test_cot['gpt-4o-mini_label'].astype(str)\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize='true'),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    # Plot both confusion matrices side by side using subplots\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a944961",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test_cot, dataset_name=dataset_name)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a956f0",
   "metadata": {},
   "source": [
    "## Environmental Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"environmental_claims\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"0\"\n",
    "positive_label = \"1\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f54d3",
   "metadata": {},
   "source": [
    "## Explicit/Implicit Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"green_claims_3\"\n",
    "\n",
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test_cot['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test_cot[test_cot['gpt-4o-mini_label'].astype(str) != test_cot['label'].astype(str)].copy()\n",
    "errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "merged_errors = errors.merge(errors_cot, on=['text', 'label'], how=\"outer\", suffixes=(\"_zero\", \"_cot\"))\n",
    "\n",
    "print(len(merged_errors[merged_errors['gpt-4o-mini_label_zero'].isna()]), len(merged_errors[merged_errors['gpt-4o-mini_label_cot'].isna()]), len(merged_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b85912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test_cot['label'].astype(str)\n",
    "predicted_labels = test_cot['gpt-4o-mini_label'].astype(str)\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize=\"true\"),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d205d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 14\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test_cot, dataset_name=dataset_name, n_sample=n_sample)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c628f",
   "metadata": {},
   "source": [
    "## Green Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"green_claims\"\n",
    "\n",
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test_cot['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test_cot[test_cot['gpt-4o-mini_label'].astype(str) != test_cot['label'].astype(str)].copy()\n",
    "errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "merged_errors = errors.merge(errors_cot, on=['text', 'label'], how=\"outer\", suffixes=(\"_zero\", \"_cot\"))\n",
    "\n",
    "print(len(merged_errors[merged_errors['gpt-4o-mini_label_zero'].isna()]), len(merged_errors[merged_errors['gpt-4o-mini_label_cot'].isna()]), len(merged_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ce82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"not_green\"\n",
    "positive_label = \"green_claim\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, negative_label=negative_label, positive_label=positive_label, dataset_name=dataset_name)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae536897",
   "metadata": {},
   "source": [
    "## ClimateBUG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe95e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climateBUG_data\"\n",
    "\n",
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test_cot['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "errors_cot = test_cot[test_cot['gpt-4o-mini_label'].astype(str) != test_cot['label'].astype(str)].copy()\n",
    "errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "merged_errors = errors.merge(errors_cot, on=['text', 'label'], how=\"outer\", suffixes=(\"_zero\", \"_cot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8626cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"non-climate\"\n",
    "positive_label = \"climate\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test_cot, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8a920",
   "metadata": {},
   "source": [
    "## ESG E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a87b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"esgbert_g\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeebc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"0\"\n",
    "positive_label = \"1\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be259945",
   "metadata": {},
   "source": [
    "## ESGBERT NAture/FOREST/water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"esgbert_category_water\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd467e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"0\"\n",
    "positive_label = \"1\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fed924",
   "metadata": {},
   "source": [
    "## Sustainable review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"sustainable_signals_review\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20fdbb",
   "metadata": {},
   "source": [
    "## ClimateEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f86c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climateEng\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test['label'].astype(str).map(label_readable_mapping['climateEng']['labels'])\n",
    "predicted_labels = test['gpt-4o-mini_label'].astype(str).map(label_readable_mapping['climateEng']['labels'])\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize=\"true\"),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    # Plot both confusion matrices side by side using subplots\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff31060",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test, dataset_name=dataset_name)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09918d6",
   "metadata": {},
   "source": [
    "## Action 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd119a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"esgbert_action500\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"0\"\n",
    "positive_label = \"1\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352525de",
   "metadata": {},
   "source": [
    "## ClimaTOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a771355",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"ClimaTOPIC\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test['label'].astype(str)\n",
    "predicted_labels = test['gpt-4o-mini_label'].astype(str)\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize='true'),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    # Plot both confusion matrices side by side using subplots\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50750665",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test, dataset_name=dataset_name)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393687b",
   "metadata": {},
   "source": [
    "## Commitment And Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5cb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climate_commitments_actions\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"no\"\n",
    "positive_label = \"yes\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e6f38",
   "metadata": {},
   "source": [
    "## Net Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"netzero_reduction\"\n",
    "\n",
    "test_cot = find_errors(dataset_name, mode=\"cot\")\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test_cot['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test_cot['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test_cot[test_cot['gpt-4o-mini_label'].astype(str) != test_cot['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e327e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test_cot['label'].astype(str)\n",
    "predicted_labels = test_cot['gpt-4o-mini_label'].astype(str)\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels)), normalize='true'),2)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    cell_width = 0.7\n",
    "    cell_height = 0.7\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    # Calculate figure size based on the number of labels and fixed cell dimensions\n",
    "    figsize = (cell_width * num_labels, cell_height * num_labels)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa29d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test_cot, dataset_name=dataset_name, n_sample=n_sample)\n",
    "get_statistics_errors_multi(dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb17e9f",
   "metadata": {},
   "source": [
    "## Climate Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climate_specificity\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_label = \"non-specific\"\n",
    "positive_label = \"specific\"\n",
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file(test, dataset_name=dataset_name, negative_label=negative_label, positive_label=positive_label)\n",
    "get_statistics_erros(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cb18f",
   "metadata": {},
   "source": [
    "## Climate Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"climate_sentiment\"\n",
    "\n",
    "test = find_errors(dataset_name)\n",
    "print(\n",
    "    classification_report(\n",
    "            y_pred=test['gpt-4o-mini_label'].astype(str), \n",
    "            y_true=test['label'].astype(str),\n",
    "            zero_division=0.0,\n",
    "        )\n",
    ")\n",
    "\n",
    "test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b62091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Replace these lists with your actual label data\n",
    "true_labels = test['label'].astype(str)\n",
    "predicted_labels = test['gpt-4o-mini_label'].astype(str)\n",
    "\n",
    "# Combine both true and predicted labels to get the full set of unique classes\n",
    "all_labels = list(set(true_labels) | set(predicted_labels))\n",
    "\n",
    "# Use LabelEncoder to encode textual labels to integers based on the full label set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(true_labels)\n",
    "predicted_labels_encoded = label_encoder.transform(predicted_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = np.round(confusion_matrix(true_labels_encoded, predicted_labels_encoded, labels=range(len(all_labels))),1)\n",
    "\n",
    "# Get the class labels back from the encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels by 45 degrees\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the confusion matrix\n",
    "plot_confusion_matrix(cm, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "\n",
    "create_jsonl_errors_file_multilabel(test, dataset_name=dataset_name)\n",
    "get_statistics_errors_multi(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f184269",
   "metadata": {},
   "source": [
    "# Table of fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ad353",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for dataset_name in ['climateFEVER_claim',\n",
    "#  'lobbymap_stance_origin',\n",
    "# 'logicClimate',\n",
    " 'sciDCC',\n",
    "#  'lobbymap_query_origin',\n",
    " 'contrarian_claims',\n",
    " 'climate_tcfd_recommendations',\n",
    " 'ClimaTOPIC',\n",
    " 'climatext',\n",
    " 'ClimaINS_ours',\n",
    " 'sustainable_signals_review',\n",
    " 'climateFEVER_evidence',\n",
    " 'climateStance',\n",
    " 'lobbymap_pages',\n",
    " 'climateEng',\n",
    " 'climate_specificity',\n",
    " 'environmental_claims',\n",
    " 'esgbert_action500',\n",
    " 'gw_stance_detection',\n",
    " 'climate_commitments_actions',\n",
    " 'climate_sentiment',\n",
    " 'green_claims_3',\n",
    " 'climateBUG_data',\n",
    " 'green_claims',\n",
    " 'esgbert_e',\n",
    " 'esgbert_category_forest',\n",
    " 'esgbert_category_nature',\n",
    " 'climaQA',\n",
    " 'esgbert_g',\n",
    " 'esgbert_category_biodiversity',\n",
    " 'esgbert_s',\n",
    " 'esgbert_category_water',\n",
    " 'climate_detection',\n",
    " 'netzero_reduction']:\n",
    "    # Find errors for GPT-4o-mini\n",
    "    test = find_errors(dataset_name)\n",
    "    gpt_errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "    # Find errors for distilRoBERTa\n",
    "    test_distilroberta = pd.read_parquet(f'experiment_results\\\\performances\\\\y_pred\\\\{dataset_name}_distilRoBERTa_42.pkl')\n",
    "    distil_errors = test_distilroberta[test_distilroberta['label'] != test_distilroberta['y_pred']].copy()\n",
    "\n",
    "    # Merge errors to find common errors\n",
    "    merged = gpt_errors.merge(distil_errors[['text', 'y_pred', 'label']], on='text', how=\"inner\", suffixes=('_gpt4', '_distil'))\n",
    "    all_errors = gpt_errors.merge(distil_errors[['text', 'y_pred', 'label']], on='text', how=\"outer\", suffixes=('_gpt4', '_distil'))\n",
    "\n",
    "    # Calculate percentages\n",
    "    gpt_error_percentage = np.round(100 * len(gpt_errors) / len(test), 2) if len(test) > 0 else 0\n",
    "    distil_error_percentage = np.round(100 * len(distil_errors) / len(test_distilroberta), 2) if len(test_distilroberta) > 0 else 0\n",
    "    common_error_percentage = np.round(100 * len(merged) / len(all_errors), 2)\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append([dataset_name.replace('_', '\\\\_'), gpt_error_percentage, distil_error_percentage, common_error_percentage])\n",
    "\n",
    "# Create a DataFrame for sorting\n",
    "results_df = pd.DataFrame(results, columns=['Dataset', 'GPT-4o-mini Error %', 'distilRoBERTa Error %', 'Common Error %'])\n",
    "\n",
    "# Sort the results by 'Common Error %' in descending order\n",
    "sorted_results_df = results_df.sort_values(by='Common Error %', ascending=False)\n",
    "\n",
    "# Print the sorted results in the desired format\n",
    "for _, row in sorted_results_df.iterrows():\n",
    "    print(f\"{row['Dataset']} & {row['GPT-4o-mini Error %']} & {row['distilRoBERTa Error %']} & {row['Common Error %']} \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(data, dataset_name):\n",
    "    outputs = []\n",
    "    for _data in data[dataset_name]:\n",
    "        outputs.append(_data[2]['content'])\n",
    "    return outputs\n",
    "\n",
    "def get_errors_llama(dataset_name):\n",
    "    results = []\n",
    "    #result_file_name = f\"llm/outputs/gpt-4o/{dataset_name}.jsonl\" if use_gpt4 else f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "    results = get_outputs(data=data, dataset_name=dataset_name)\n",
    "    \n",
    "    labels = []\n",
    "    explainations = []\n",
    "    \n",
    "    for result in results:\n",
    "        label, explanation = parse_label_explanation(result)\n",
    "        labels += [label]\n",
    "        explainations += [explanation]\n",
    "    \n",
    "    # test = pd.read_parquet(os.path.join(\"doccano\", \"random\", \"parquet\", f\"{dataset_name}.pkl\"))\n",
    "    test = pd.read_parquet(f'doccano\\\\random\\\\parquet\\\\{dataset_name}.pkl')\n",
    "        \n",
    "    if dataset_name == \"lobbymap_pages\":\n",
    "        test['label'] = 1 * test['label']\n",
    "\n",
    "    test['model_label'] = labels\n",
    "    test['model_explanation'] = explainations\n",
    "\n",
    "    if dataset_name == \"lobbymap_query_origin\":\n",
    "        test = test[test['query'].astype(str) != \"[None]\"]\n",
    "        dataset_name = \"lobbymap_query\"\n",
    "    if dataset_name == \"lobbymap_stance_origin\":\n",
    "        test = test[test['query'].astype(str) != \"None\"]\n",
    "        test.rename(columns={'stance':'label'}, inplace=True)\n",
    "        dataset_name = \"lobbymap_stance\"\n",
    "\n",
    "    if dataset_name in label_readable_mapping:\n",
    "        label2id = {v.lower(): k for k, v in label_readable_mapping[dataset_name]['labels'].items()}\n",
    "        test['model_label'] = test['model_label'].str.lower().map(label2id)  \n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa84573",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# for dataset_name in ['climateFEVER_claim',\n",
    "# #  'lobbymap_stance_origin',\n",
    "# # 'logicClimate',\n",
    "#  'sciDCC',\n",
    "# #  'lobbymap_query_origin',\n",
    "#  'contrarian_claims',\n",
    "#  'climate_tcfd_recommendations',\n",
    "#  'ClimaTOPIC',\n",
    "#  'climatext',\n",
    "#  'ClimaINS_ours',\n",
    "#  'sustainable_signals_review',\n",
    "# #  'climateFEVER_evidence',\n",
    "#  'climateStance',\n",
    "#  'lobbymap_pages',\n",
    "#  'climateEng',\n",
    "#  'climate_specificity',\n",
    "#  'environmental_claims',\n",
    "#  'esgbert_action500',\n",
    "#  'gw_stance_detection',\n",
    "#  'climate_commitments_actions',\n",
    "#  'climate_sentiment',\n",
    "#  'green_claims_3',\n",
    "#  'climateBUG_data',\n",
    "#  'green_claims',\n",
    "#  'esgbert_e',\n",
    "#  'esgbert_category_forest',\n",
    "#  'esgbert_category_nature',\n",
    "#  #'climaQA',\n",
    "#  'esgbert_g',\n",
    "#  'esgbert_category_biodiversity',\n",
    "#  'esgbert_s',\n",
    "#  'esgbert_category_water',\n",
    "#  'climate_detection',\n",
    "#  'netzero_reduction']:\n",
    "for dataset_name in ['netzero_reduction']:\n",
    "    print(\"#\"*10)\n",
    "    print(dataset_name)\n",
    "    print(\"#\"*10)\n",
    "\n",
    "    # Find errors for GPT-4o-mini\n",
    "    test = find_errors(dataset_name)\n",
    "    gpt_errors = test[test['gpt-4o-mini_label'].astype(str) != test['label'].astype(str)].copy()\n",
    "\n",
    "    # Find errors for distilRoBERTa\n",
    "    test_distilroberta = pd.read_parquet(f'experiment_results\\\\performances\\\\y_pred\\\\{dataset_name}_distilRoBERTa_42.pkl')\n",
    "    distil_errors = test_distilroberta[test_distilroberta['label'] != test_distilroberta['y_pred']].copy()\n",
    "\n",
    "    # Find errors for Llama\n",
    "    test_llama = get_errors_llama(dataset_name)\n",
    "    llama_errors = test_llama[test_llama['label'].astype(str) != test_llama['model_label'].astype(str)].copy()\n",
    "\n",
    "    # Merge errors to find common errors\n",
    "    merged = gpt_errors.merge(distil_errors[['text', 'y_pred', 'label']], on='text', how=\"inner\", suffixes=('_gpt4', '_distil'))\n",
    "    all_errors = gpt_errors.merge(distil_errors[['text', 'y_pred', 'label']], on='text', how=\"outer\", suffixes=('_gpt4', '_distil'))\n",
    "    all_errors = test[['text']].merge(all_errors, on='text', how='inner')\n",
    "\n",
    "    merged_llama_gpt = gpt_errors.merge(llama_errors[['text', 'model_label', 'model_explanation', 'label']], on='text', how=\"inner\", suffixes=('_gpt4', '_llama'))\n",
    "    all_errors_llama = gpt_errors.merge(llama_errors[['text', 'model_label', 'model_explanation', 'label']], on='text', how=\"outer\", suffixes=('_gpt4', '_llama'))\n",
    "    common_test = test_llama[['text']].merge(test[['text']], how=\"inner\", on=\"text\")\n",
    "    all_errors_llama = common_test[['text']].merge(all_errors_llama, on='text', how='inner')\n",
    "\n",
    "    # Calculate percentages\n",
    "    gpt_error_percentage = np.round(100 * len(gpt_errors) / len(test), 2) if len(test) > 0 else 0\n",
    "    distil_error_percentage = np.round(100 * len(distil_errors) / len(test_distilroberta), 2) if len(test_distilroberta) > 0 else 0\n",
    "    common_error_percentage = np.round(100 * len(merged) / len(all_errors), 2) if len(all_errors) > 0 else np.nan\n",
    "\n",
    "    llama_error_percentage = np.round(100 * len(llama_errors) / len(test_llama), 2) if len(test_llama) > 0 else 0\n",
    "    llama_common_error_percentage = np.round(100 * len(merged_llama_gpt) / len(all_errors_llama), 2) if len(all_errors_llama) > 0 else np.nan\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append([dataset_name.replace('_', '\\\\_'), gpt_error_percentage, distil_error_percentage, common_error_percentage, llama_error_percentage, llama_common_error_percentage])\n",
    "\n",
    "# Create a DataFrame for sorting\n",
    "results_df = pd.DataFrame(results, columns=['Dataset', 'GPT-4o-mini Error %', 'distilRoBERTa Error %', 'GPT-4o-mini-DistilBERT Common Error %', 'Llama Error %', 'Llama-GPT-4omini Common %'])\n",
    "\n",
    "# Sort the results by 'Common Error %' in descending order\n",
    "sorted_results_df = results_df.sort_values(by='GPT-4o-mini-DistilBERT Common Error %', ascending=False)\n",
    "\n",
    "# Print the sorted results in the desired format\n",
    "for _, row in sorted_results_df.iterrows():\n",
    "    print(f\"{row['Dataset']} & {row['GPT-4o-mini Error %']} & {row['distilRoBERTa Error %']} & {row['GPT-4o-mini-DistilBERT Common Error %']} & {row['Llama Error %']} & {row['Llama-GPT-4omini Common %']} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a85ab0",
   "metadata": {},
   "source": [
    "# Parse LLama outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffcd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_label_explanation(text, dataset_name=None):\n",
    "    try:\n",
    "        # Regular expression to extract Label and Explanation\n",
    "        label_pattern = r'Label:\\s*(.*)'\n",
    "        explanation_pattern = r'Explanation:\\s*(.*)'\n",
    "\n",
    "        # Find the label\n",
    "        label_match = re.search(label_pattern, text)\n",
    "        label = label_match.group(1) if label_match else None\n",
    "\n",
    "        # Find the explanation\n",
    "        explanation_match = re.search(explanation_pattern, text, re.DOTALL)\n",
    "        explanation = explanation_match.group(1).strip() if explanation_match else None\n",
    "        \n",
    "        label = label.replace('[', \"\").replace(']', \"\").strip()\n",
    "\n",
    "        label = label.replace('Climate solutions won’t work, Climate policies are harmful', \"Climate solutions won’t work, Climate policies (mitigation or adaptation) are harmful\")\n",
    "        label = label.replace('Climate solutions won’t work, One country is negligible', 'Climate solutions won’t work, Climate policies are ineffective/flawed')\n",
    "\n",
    "        if dataset_name == \"ClimaINS_ours\":\n",
    "            label = label.split(\" \")[0]\n",
    "        elif dataset_name == \"climate_tcfd_recommendations\":\n",
    "            label = label.replace(\"general\", \"none\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        label = None\n",
    "        if dataset_name == \"netzero_reduction\":\n",
    "            label=\"none\"\n",
    "        elif dataset_name == \"lobbymap_pages\":\n",
    "            label=\"The page does not contain evidence about the stance of the company regarding any of the policy\"\n",
    "        elif dataset_name == \"logicClimate\":\n",
    "            label = \"None\"\n",
    "        explanation = e\n",
    "\n",
    "    return label, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402eb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src.logger import bootstrap_confidence_interval_saving\n",
    "\n",
    "llama_path = \"llm\\outputs\\llama\\\\results_climatext_llama_8B_cot.json\" # \"llm\\outputs\\llama\\greenbench_llama_70B_cot_4bit.json\"\n",
    "model_type = \"Llama-8B\" # Llama-70B\n",
    "\n",
    "# Read the JSON file as a string\n",
    "with open(llama_path, 'r') as file:\n",
    "    json_string = file.read()\n",
    "\n",
    "# Parse the JSON string into a dictionary\n",
    "data = json.loads(json_string)\n",
    "data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21327ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(data, dataset_name):\n",
    "    outputs = []\n",
    "    for _data in data[dataset_name]:\n",
    "        outputs.append(_data[2]['content'])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_file_path = \"experiment_results\\performances\\performances_llama.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "performance_type = \"f1_score\"\n",
    "\n",
    "if os.path.exists(perf_file_path):\n",
    "    performances = pd.read_csv(perf_file_path)\n",
    "else:\n",
    "    performances = pd.DataFrame()\n",
    "\n",
    "for dataset_name in set(prompts.keys())-{\"lobbymap_query\", \"logicClimate\"}:\n",
    "    # Loading data from saved file\n",
    "    results = []\n",
    "    #result_file_name = f\"llm/outputs/gpt-4o/{dataset_name}.jsonl\" if use_gpt4 else f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "    results = get_outputs(data=data, dataset_name=dataset_name)\n",
    "    \n",
    "    labels = []\n",
    "    explainations = []\n",
    "    \n",
    "    for result in results:\n",
    "        label, explanation = parse_label_explanation(result)\n",
    "        labels += [label]\n",
    "        explainations += [explanation]\n",
    "    \n",
    "    test = pd.read_parquet(f'data\\\\llm_green_nlp_tasks\\\\{dataset_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f408419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "performance_type = \"f1_score\"\n",
    "\n",
    "if os.path.exists(perf_file_path):\n",
    "    performances = pd.read_csv(perf_file_path)\n",
    "else:\n",
    "    performances = pd.DataFrame()\n",
    "\n",
    "# set(prompts.keys())-{\"lobbymap_query\", \"logicClimate\"}\n",
    "# for dataset_name in set(prompts.keys())-{\"lobbymap_query\", \"logicClimate\", \"lobbymap\", 'climatext_10k', \"lobbymap_stance\"}:\n",
    "for dataset_name in [\"climatext_10k\", \"climatext_wiki\", \"climatext_claim\"]:\n",
    "\n",
    "    # if dataset_name == \"map_lobbymap_stance\":\n",
    "    #     continue\n",
    "    \n",
    "    # Loading data from saved file\n",
    "    results = []\n",
    "    #result_file_name = f\"llm/outputs/gpt-4o/{dataset_name}.jsonl\" if use_gpt4 else f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "    results = get_outputs(data=data, dataset_name=dataset_name)\n",
    "    \n",
    "    labels = []\n",
    "    explainations = []\n",
    "    \n",
    "    for result in results:\n",
    "        label, explanation = parse_label_explanation(result, dataset_name=dataset_name)\n",
    "        labels += [label]\n",
    "        explainations += [explanation]\n",
    "    \n",
    "    # test = pd.read_parquet(os.path.join(\"doccano\", \"random\", \"parquet\", f\"{dataset_name}.pkl\"))\n",
    "    # test = pd.read_parquet(f'doccano\\\\random\\\\parquet\\\\{dataset_name}.pkl')\n",
    "    test = pd.read_parquet(f'data\\\\llm_green_nlp_tasks\\\\{dataset_name}.pkl')\n",
    "\n",
    "    if dataset_name == \"lobbymap_pages\":\n",
    "        test['label'] = 1 * test['label']\n",
    "\n",
    "    test['model_label'] = labels\n",
    "    test['model_explanation'] = explainations\n",
    "\n",
    "    if dataset_name == \"sustainable_signals_review\":\n",
    "        test['model_label'] = test['model_label'].str.replace(\"Not Relevant\", \"Not relevant\")\n",
    "    \n",
    "    if dataset_name == \"contrarian_claims\":\n",
    "        test['model_label'] = test['model_label'].str.replace(\"No claim\", \"No claim, No claim\")\n",
    "\n",
    "    if dataset_name == \"lobbymap_query_origin\":\n",
    "        test = test[test['query'].astype(str) != \"[None]\"]\n",
    "        dataset_name = \"lobbymap_query\"\n",
    "    if dataset_name == \"lobbymap_stance_origin\":\n",
    "        test = test[test['query'].astype(str) != \"None\"]\n",
    "        test.rename(columns={'stance':'label'}, inplace=True)\n",
    "        dataset_name = \"lobbymap_stance\"\n",
    "    \n",
    "    print(dataset_name)\n",
    "    \n",
    "    if dataset_name in [\"logicClimate\", \"lobbymap_query\"]:\n",
    "        if dataset_name == \"logicClimate\":\n",
    "            y_true = test['label'].apply(literal_eval)\n",
    "            y_pred = test['model_label'].apply(lambda x: [e.strip().lower() for e in x.split(\",\")])\n",
    "        elif dataset_name == \"lobbymap_query\":\n",
    "            y_true = test['label'].apply(lambda x: [map_lobbymap_stance[e] for e in x])\n",
    "            y_pred = test['model_label'].apply(lambda x: [e.strip() for e in x.split(\",\")])   \n",
    "        \n",
    "        # Initialize the MultiLabelBinarizer\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        \n",
    "        # Fit the binarizer and transform the labels\n",
    "        y_true_binarized = mlb.fit_transform(y_true)\n",
    "        y_pred_binarized = mlb.transform(y_pred)\n",
    "        \n",
    "        report = classification_report(\n",
    "                    y_pred=y_pred_binarized, \n",
    "                    y_true=y_true_binarized,\n",
    "                    target_names=mlb.classes_,\n",
    "                    zero_division=0.0,\n",
    "                    output_dict=True \n",
    "                )\n",
    "        \n",
    "        f1_lower, f1_upper = bootstrap_confidence_interval_saving(y_pred=y_pred_binarized, y_true=y_true_binarized, num_bootstrap_samples=1000, dataset_name=dataset_name, model_name=model_type)\n",
    "        print(report['macro avg']['f1-score'], f1_lower, f1_upper)\n",
    "        \n",
    "    else:\n",
    "        if dataset_name in label_readable_mapping:\n",
    "            label2id = {v.lower(): k for k, v in label_readable_mapping[dataset_name]['labels'].items()}\n",
    "            test['model_label'] = test['model_label'].str.lower().map(label2id)\n",
    "\n",
    "        # Replace non existing labels by the most frequent\n",
    "        test.loc[~test['model_label'].astype(str).isin(test['label'].astype(str).unique()), \"model_label\"] = test['label'].mode()[0]\n",
    "                    \n",
    "        report = classification_report(\n",
    "                y_pred=test['model_label'].astype(str), \n",
    "                y_true=test['label'].astype(str),\n",
    "                zero_division=0.0,\n",
    "                output_dict=True            \n",
    "            )\n",
    "        \n",
    "        f1_lower, f1_upper = bootstrap_confidence_interval_saving(y_pred=test['model_label'].astype(str), y_true=test['label'].astype(str), num_bootstrap_samples=1000, dataset_name=dataset_name, model_name=model_type)\n",
    "        print(report['macro avg']['f1-score'], f1_lower, f1_upper)\n",
    "        \n",
    "    if ('samples avg' in report.keys()) and ('accuracy' not in report.keys()):\n",
    "        report['accuracy'] = report['samples avg']['f1-score']\n",
    "        \n",
    "    new_row = pd.DataFrame({\n",
    "        'dataset_name': [dataset_name],\n",
    "        'model_type': [model_type],\n",
    "        'performance': [report['macro avg']['f1-score']],\n",
    "        'performance_type': [performance_type],\n",
    "        'n_labels': [np.nan],\n",
    "        'seed': [42],\n",
    "        \"f1_upper\": [f1_upper],\n",
    "        \"f1_lower\": [f1_lower],\n",
    "        \"n_epoch\": [np.nan],\n",
    "        \"precision\": [report['macro avg']['precision']],\n",
    "        \"recall\": [report['macro avg']['recall']],\n",
    "        \"weighted_f1\": [report['weighted avg']['f1-score']],\n",
    "        \"accuracy\": [report['accuracy']]\n",
    "    })\n",
    "    performances = pd.concat([performances, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances.to_csv(perf_file_path, index=False)\n",
    "performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d2b5d",
   "metadata": {},
   "source": [
    "# Meta Analysis of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11046786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "path = \"paper_utils/error_analysis/\"\n",
    "\n",
    "error_analysis_df = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if \"_old\" in filename:\n",
    "        continue\n",
    "    if \"archive\" in filename:\n",
    "        continue\n",
    "    if \".zip\" in filename:\n",
    "        continue\n",
    "    if \"iaa\" in filename:\n",
    "        continue\n",
    "\n",
    "    dataset_name = filename[:-8]\n",
    "\n",
    "    file_df = pd.read_json(path+filename)\n",
    "    file_df['dataset_name'] = dataset_name\n",
    "\n",
    "    if \"comment_page\" in file_df:\n",
    "        file_df = file_df[file_df[\"comment_page\"]!=\"true:\"].copy()\n",
    "        file_df = file_df[file_df[\"comment_page\"]!=\"\"].copy()\n",
    "        file_df.rename(columns={\"comment_query\":\"comment\"}, inplace=True)\n",
    "\n",
    "    error_analysis_df = pd.concat([error_analysis_df, file_df[['comment', 'dataset_name']]], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b034ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_errors = dict()\n",
    "\n",
    "cot_list = [\"netzero_reduction\", \n",
    "\"climate_tcfd_recommendations\",\n",
    "\"climateBUG_data\",\n",
    "\"green_claims\",\n",
    "\"green_claims_3\",\n",
    "\"climateStance\",\n",
    "\"gw_stance_detection\",\n",
    "\"climateFEVER_evidence\",\n",
    "\"lobbymap_query\",\n",
    "\"lobbymap_stance\",\n",
    "\"lobbymap_pages\",\n",
    "\"climatext\",\n",
    "\"climatext_10k\",\n",
    "\"climatext_claim\",\n",
    "\"climatext_wiki\",\n",
    "\"climate_detection\"]\n",
    "\n",
    "l = []\n",
    "for filename in os.listdir(path):\n",
    "    if \"lobbymap\" in filename:\n",
    "        continue\n",
    "    if \"_old\" in filename:\n",
    "        continue\n",
    "    if \"archive\" in filename:\n",
    "        continue\n",
    "    if \".zip\" in filename:\n",
    "        continue\n",
    "    if \"iaa\" in filename:\n",
    "        continue\n",
    "\n",
    "    ds_name = filename.replace(\"_fn.json\", \"\").replace(\"_fp.json\", \"\")\n",
    "    l += [ds_name]\n",
    "\n",
    "for ds_name in set(l):\n",
    "    if ds_name in cot_list:\n",
    "        test = find_errors(ds_name, mode=\"cot\")\n",
    "    else:\n",
    "        test = find_errors(ds_name)\n",
    "\n",
    "    number_errors[ds_name] = len(test[test['gpt-4o-mini_label'].astype(str)!=test['label'].astype(str)])\n",
    "    # print(test[test['gpt-4o-mini_label'].astype(str)!=test['label'].astype(str)].sample(1)[['gpt-4o-mini_label', 'label']])\n",
    "\n",
    "test = find_errors(\"lobbymap_pages\", mode=\"cot\")\n",
    "number_errors['lobbymap_query_p'] = len(test[(1*test['label']).astype(str) != test['gpt-4o-mini_label'].astype(str)])\n",
    "\n",
    "test = find_errors(\"lobbymap_stance_origin\")\n",
    "test = test[~test['stance'].isna()]\n",
    "number_errors['lobbymap_query_stance'] = len(test[test['stance'].astype(str) != test['gpt-4o-mini_label'].astype(str)])\n",
    "\n",
    "number_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis_df['error_annotation'] = error_analysis_df['comment'].apply(lambda x: x.split(\":\")[0].split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcdb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_error_types = error_analysis_df['error_annotation'].explode().unique()\n",
    "unique_error_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis_df['error_annotation'] = error_analysis_df['comment'].apply(lambda x: x.split(\":\")[0].split(\",\"))\n",
    "\n",
    "unique_error_types = error_analysis_df['error_annotation'].explode().unique()\n",
    "\n",
    "error_analysis_df[\"Actual Error\"] = error_analysis_df['error_annotation'].apply(\n",
    "    lambda x: (\"error\" in x) | (\"honest-mistake\" in x) | (\"indirect\" in x) | (\"implicit-describe\" in x) | (\"table\" in x) | (\"list\" in x) # Check for last one (update annotations)\n",
    "                                                                                )\n",
    "def debtable(x):\n",
    "    if \"ambiguous\" in x:\n",
    "        return True\n",
    "    if \"main-branch\" in x:\n",
    "        return True\n",
    "    if \"cropped-small\" in x:\n",
    "        return True\n",
    "    if \"off-topic\" in x:\n",
    "        return True\n",
    "    if \"debatable\" in x:\n",
    "        return True\n",
    "    if \"out-of-context\" in x: # Check if error/ooo or debatalbe only etc ?\n",
    "        return True\n",
    "    if \"close\" in x:\n",
    "        return True\n",
    "    if \"page-selection\" in x:\n",
    "        return True\n",
    "    if \"nature-arg\" in x: # Sure ?\n",
    "        return True\n",
    "    if \"exhaustif\" in x:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "error_analysis_df[\"Debatable Error\"] = error_analysis_df['error_annotation'].apply(debtable)\n",
    "error_analysis_df[\"Multilabel\"] = error_analysis_df['error_annotation'].apply(lambda x: (\"multi\" in x) | (\"multiple\" in x))\n",
    "error_analysis_df[\"Mislabeled\"] = error_analysis_df['error_annotation'].apply(lambda x: \"wrong\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_error_analysis = error_analysis_df[(error_analysis_df['Actual Error'] | error_analysis_df['Debatable Error'] | error_analysis_df['Mislabeled'] | error_analysis_df['Multilabel'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ae229",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_error_analysis.loc[annotated_error_analysis['Debatable Error'], \"error\"] = \"Debatable Error\"\n",
    "annotated_error_analysis.loc[annotated_error_analysis['Multilabel'], \"error\"] = \"Multilabel\"\n",
    "annotated_error_analysis.loc[annotated_error_analysis['Actual Error'], \"error\"] = \"Actual Error\"\n",
    "annotated_error_analysis.loc[annotated_error_analysis['Mislabeled'], \"error\"] = \"Mislabeled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors = number_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec182fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_labels = dict()\n",
    "for ds_name in annotated_error_analysis['dataset_name']:\n",
    "    mapping_labels[ds_name] = parse_dataset_name(ds_name)\n",
    "\n",
    "mapping_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'number_errors' and 'total_errors' are defined and contain the total number of errors per dataset.\n",
    "error_counts_per_dataset = annotated_error_analysis.groupby(['dataset_name', 'error']).size().unstack(fill_value=0)\n",
    "\n",
    "# Add 'Not Annotated' column\n",
    "error_counts_per_dataset['Not Annotated'] = error_counts_per_dataset.index.map(number_errors)\n",
    "error_counts_per_dataset['Not Annotated'] = error_counts_per_dataset['Not Annotated'] - error_counts_per_dataset['Actual Error'] - error_counts_per_dataset['Debatable Error'] - error_counts_per_dataset['Mislabeled'] - error_counts_per_dataset['Multilabel']\n",
    "\n",
    "error_type_colors = {\n",
    "    'Actual Error':  '#3498DB',   # Blue (neutral)\n",
    "    'Debatable Error': '#F39C12',  # Orange (ambiguous)\n",
    "    'Mislabeled': '#E74C3C', # Red (critical)\n",
    "    'Multilabel': '#9B59B6',  # Purple (distinct)\n",
    "    'Not Annotated': '#95A5A6',  # Gray (uncertain)\n",
    "}\n",
    "\n",
    "error_counts_per_dataset.index = error_counts_per_dataset.index.map(mapping_labels)\n",
    "\n",
    "\n",
    "# Create a figure with 2 rows (one for the total errors and one for the stacked bars)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "# Plotting the total number of errors (this will be above the stacked bar plot)\n",
    "error_counts_per_dataset['Total Errors'] = error_counts_per_dataset.sum(axis=1)\n",
    "\n",
    "# Plotting the stacked bar plot below with custom colors\n",
    "error_types = error_counts_per_dataset.drop('Total Errors', axis=1).columns\n",
    "colors = [error_type_colors[error_type] for error_type in error_types]\n",
    "\n",
    "ax1.bar(error_counts_per_dataset.index, error_counts_per_dataset['Total Errors'], color=error_type_colors['Not Annotated'], width=0.5)\n",
    "ax1.set_title('Total Number of Errors per Dataset')\n",
    "ax1.set_ylabel('Total Errors')\n",
    "ax1.set_ylim(25, 665)\n",
    "\n",
    "\n",
    "# Plotting the stacked bar plot below\n",
    "error_counts_per_dataset.drop('Total Errors', axis=1).plot(kind='bar', stacked=True, color=colors, ax=ax2)\n",
    "\n",
    "ax2.set_title('Error Types per Dataset')\n",
    "ax2.set_xlabel('Dataset Name')\n",
    "ax2.set_ylim(0, 25)\n",
    "ax2.set_ylabel('Count of Error Types')\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "ax2.set_xticklabels(error_counts_per_dataset.index, rotation=90)\n",
    "\n",
    "# Add a legend\n",
    "ax2.legend(title=\"Error Type\", bbox_to_anchor=(0.5, -1), loc='upper center', ncol=len(error_types), frameon=False)\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9450a78",
   "metadata": {},
   "source": [
    "# IAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relax_annotation(comment):\n",
    "    x =  comment.split(\":\")[0].split(\",\")\n",
    "\n",
    "    Error = (\"error\" in x) | (\"honest-mistake\" in x) | (\"indirect\" in x) | (\"implicit-describe\" in x) | (\"table\" in x) | (\"list\" in x)\n",
    "    \n",
    "    def debtable(x):\n",
    "        if \"ambiguous\" in x:\n",
    "            return True\n",
    "        if \"main-branch\" in x:\n",
    "            return True\n",
    "        if \"cropped-small\" in x:\n",
    "            return True\n",
    "        if \"off-topic\" in x:\n",
    "            return True\n",
    "        if \"debatable\" in x:\n",
    "            return True\n",
    "        if \"out-of-context\" in x: # Check if error/ooo or debatalbe only etc ?\n",
    "            return True\n",
    "        if \"close\" in x:\n",
    "            return True\n",
    "        if \"page-selection\" in x:\n",
    "            return True\n",
    "        if \"nature-arg\" in x: # Sure ?\n",
    "            return True\n",
    "        if \"exhaustif\" in x:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    Debatable = debtable(x)\n",
    "    Multilabel = (\"multi\" in x) | (\"multiple\" in x)\n",
    "    Wrong = \"wrong\" in x\n",
    "\n",
    "    if Wrong:\n",
    "        return \"wrong\"\n",
    "    elif Error:\n",
    "        return \"error\"\n",
    "    elif Multilabel:\n",
    "        return \"multi\"\n",
    "    else:\n",
    "        return \"debatable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "path_1 = \"error_analysis/\"\n",
    "path_2 = \"error_analysis/annotator2/\"\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(path_2):\n",
    "    print(filename)\n",
    "    \n",
    "    with open(path_1+filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        data_1 = pd.read_json(file)\n",
    "        data_1['error_type']=data_1['comment'].apply(relax_annotation)\n",
    "        data_1[\"dataset\"] = filename[:-8]\n",
    "\n",
    "    with open(path_2+filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        data_2 = pd.read_json(file)\n",
    "        data_2['error_type']=data_2['comment'].apply(relax_annotation)\n",
    "        data_2[\"dataset\"] = filename[:-8]\n",
    "\n",
    "    data = data_1.merge(data_2, how=\"left\", on=['text', 'label', 'gpt-4o-mini_label', 'gpt-4o-mini_explanation', \"dataset\"], suffixes=(\"_1\", \"_2\"))\n",
    "    all_data = pd.concat([all_data, data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9decbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "df = all_data[[\"dataset\", \"error_type_1\", \"error_type_2\"]].copy()  # Replace with your actual file path\n",
    "\n",
    "df = df[df[\"dataset\"].isin(['climateEng', 'climate_detection', 'climate_sentiment', 'green_claims_3'])].copy()\n",
    "\n",
    "# Assuming columns are named \"annotator_1\" and \"annotator_2\"\n",
    "labels = [\"error\", \"wrong\", \"multi\", \"debatable\"]\n",
    "\n",
    "# Compute Cohen's Kappa\n",
    "kappa = cohen_kappa_score(df[\"error_type_1\"], df[\"error_type_2\"], labels=labels)\n",
    "print(f\"Cohen's Kappa: {kappa:.3f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(df[\"error_type_1\"], df[\"error_type_2\"], labels=labels)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Annotator 2\")\n",
    "plt.ylabel(\"Annotator 1\")\n",
    "plt.title(\"Confusion Matrix of Annotations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f88c68",
   "metadata": {},
   "source": [
    "# Comparing LLM errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82cba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cot_list = [\"netzero_reduction\", \n",
    "\"climate_tcfd_recommendations\",\n",
    "\"climateBUG_data\",\n",
    "\"green_claims\",\n",
    "\"green_claims_3\",\n",
    "\"climateStance\",\n",
    "\"gw_stance_detection\",\n",
    "\"climateFEVER_evidence\",\n",
    "\"lobbymap_query\",\n",
    "\"lobbymap_stance\",\n",
    "\"lobbymap_pages\",\n",
    "\"climatext\",\n",
    "\"climatext_10k\",\n",
    "\"climatext_claim\",\n",
    "\"climatext_wiki\",\n",
    "\"climate_detection\"]\n",
    "\n",
    "path = \"paper_utils/error_analysis/\"\n",
    "\n",
    "l = []\n",
    "for filename in os.listdir(path):\n",
    "    if \"iaa\" in filename:\n",
    "        continue\n",
    "    if \"lobbymap\" in filename:\n",
    "        continue\n",
    "    if \"_old\" in filename:\n",
    "        continue\n",
    "    if \"archive\" in filename:\n",
    "        continue\n",
    "    if \".zip\" in filename:\n",
    "        continue\n",
    "    if \"FEVER\" in filename:\n",
    "        continue\n",
    "    if \"ClimaQA\" in filename:\n",
    "        continue\n",
    "\n",
    "    ds_name = filename.replace(\"_fn.json\", \"\").replace(\"_fp.json\", \"\")\n",
    "    l += [ds_name]\n",
    "\n",
    "for ds_name in set(l):\n",
    "    if ds_name in cot_list:\n",
    "        test = find_errors(ds_name, mode=\"cot\")\n",
    "    else:\n",
    "        test = find_errors(ds_name)\n",
    "\n",
    "    annotated_errors = pd.DataFrame()\n",
    "    if os.path.exists(path + ds_name + \"_fp.json\"):\n",
    "        annotated_errors = pd.concat([annotated_errors, pd.read_json(path + ds_name + \"_fp.json\")])\n",
    "    if os.path.exists(path + ds_name + \"_fn.json\"):\n",
    "        annotated_errors = pd.concat([annotated_errors, pd.read_json(path + ds_name + \"_fn.json\")])\n",
    "\n",
    "    annotated_errors['error_annotation'] = annotated_errors['comment'].apply(lambda x: x.split(\":\")[0].split(\",\"))\n",
    "    annotated_errors[\"Multilabel\"] = annotated_errors['error_annotation'].apply(lambda x: (\"multi\" in x) | (\"multiple\" in x))\n",
    "    annotated_errors[\"Mislabeled\"] = annotated_errors['error_annotation'].apply(lambda x: \"wrong\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src.logger import bootstrap_confidence_interval_saving\n",
    "\n",
    "llama_path = \"llm\\outputs\\llama\\greenbench_llama_8B_cot.json\" #\"llm\\outputs\\llama\\\\results_climatext_llama_8B_cot.json\" # \"llm\\outputs\\llama\\greenbench_llama_70B_cot_4bit.json\"\n",
    "llama_path_climatext = \"llm\\outputs\\llama\\\\results_climatext_llama_8B_cot.json\"\n",
    "model_type = \"Llama-8B\" # Llama-70B\n",
    "\n",
    "# Read the JSON file as a string\n",
    "with open(llama_path, 'r') as file:\n",
    "    json_string = file.read()\n",
    "\n",
    "# Parse the JSON string into a dictionary\n",
    "data = json.loads(json_string)\n",
    "data = json.loads(data)\n",
    "\n",
    "with open(llama_path_climatext, 'r') as file:\n",
    "    json_string = file.read()\n",
    "\n",
    "data_clima = json.loads(json_string)\n",
    "data_clima = json.loads(data_clima)\n",
    "\n",
    "data = {**data, **data_clima}\n",
    "\n",
    "def get_outputs(data, dataset_name):\n",
    "    outputs = []\n",
    "    for _data in data[dataset_name]:\n",
    "        outputs.append(_data[2]['content'])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f742969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llama_path = \"llm\\outputs\\llama\\greenbench_llama_70B_cot_4bit.json\" #\"llm\\outputs\\llama\\\\results_climatext_llama_8B_cot.json\" # \"llm\\outputs\\llama\\greenbench_llama_70B_cot_4bit.json\"\n",
    "llama_path_climatext = \"llm\\outputs\\llama\\\\results_climatext_llama_70B_cot.json\"\n",
    "model_type = \"Llama-8B\" # Llama-70B\n",
    "\n",
    "# Read the JSON file as a string\n",
    "with open(llama_path, 'r') as file:\n",
    "    json_string = file.read()\n",
    "\n",
    "# Parse the JSON string into a dictionary\n",
    "data_70b = json.loads(json_string)\n",
    "data_70b = json.loads(data_70b)\n",
    "\n",
    "with open(llama_path_climatext, 'r') as file:\n",
    "    json_string = file.read()\n",
    "\n",
    "data_clima = json.loads(json_string)\n",
    "data_clima = json.loads(data_clima)\n",
    "\n",
    "data_70b = {**data_70b, **data_clima}\n",
    "\n",
    "def get_outputs(data, dataset_name):\n",
    "    outputs = []\n",
    "    for _data in data[dataset_name]:\n",
    "        outputs.append(_data[2]['content'])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb28949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llama(data, dataset_name):\n",
    "    # Loading data from saved file\n",
    "    results = []\n",
    "    #result_file_name = f\"llm/outputs/gpt-4o/{dataset_name}.jsonl\" if use_gpt4 else f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "    results = get_outputs(data=data, dataset_name=dataset_name)\n",
    "    \n",
    "    labels = []\n",
    "    explainations = []\n",
    "    \n",
    "    for result in results:\n",
    "        label, explanation = parse_label_explanation(result, dataset_name=dataset_name)\n",
    "        labels += [label]\n",
    "        explainations += [explanation]\n",
    "    \n",
    "    # test = pd.read_parquet(os.path.join(\"doccano\", \"random\", \"parquet\", f\"{dataset_name}.pkl\"))\n",
    "    # test = pd.read_parquet(f'doccano\\\\random\\\\parquet\\\\{dataset_name}.pkl')\n",
    "    test = pd.read_parquet(f'data\\\\llm_green_nlp_tasks\\\\{dataset_name}.pkl')\n",
    "\n",
    "\n",
    "    if dataset_name == \"lobbymap_pages\":\n",
    "        test['label'] = 1 * test['label']\n",
    "\n",
    "    test['model_label'] = labels\n",
    "    test['model_explanation'] = explainations\n",
    "\n",
    "    if dataset_name == \"sustainable_signals_review\":\n",
    "        test['model_label'] = test['model_label'].str.replace(\"Not Relevant\", \"Not relevant\")\n",
    "    \n",
    "    if dataset_name == \"contrarian_claims\":\n",
    "        test['model_label'] = test['model_label'].str.replace(\"No claim\", \"No claim, No claim\")\n",
    "\n",
    "    if dataset_name == \"lobbymap_query_origin\":\n",
    "        test = test[test['query'].astype(str) != \"[None]\"]\n",
    "        dataset_name = \"lobbymap_query\"\n",
    "    if dataset_name == \"lobbymap_stance_origin\":\n",
    "        test = test[test['query'].astype(str) != \"None\"]\n",
    "        test.rename(columns={'stance':'label'}, inplace=True)\n",
    "        dataset_name = \"lobbymap_stance\"\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "total_llama = 0\n",
    "total_llama_70b = 0\n",
    "total_gpt = 0\n",
    "total_errors = 0\n",
    "Total_common_8b = 0\n",
    "Total_8b = 0\n",
    "Total_common_70b = 0\n",
    "Total_70b = 0\n",
    "\n",
    "performance_type = \"f1_score\"\n",
    "\n",
    "if os.path.exists(perf_file_path):\n",
    "    performances = pd.read_csv(perf_file_path)\n",
    "else:\n",
    "    performances = pd.DataFrame()\n",
    "\n",
    "print(\"dataset & Llama 8B & Llama 70B & Sampled & Llama 8B & Llama 70B \\\\\")\n",
    "\n",
    "# set(prompts.keys())-{\"lobbymap_query\", \"logicClimate\"}\n",
    "# for dataset_name in set(prompts.keys())-{\"lobbymap_query\", \"logicClimate\", \"lobbymap\", 'climatext_10k', \"lobbymap_stance\"}:\n",
    "for dataset_name in set(prompts.keys())-{\"lobbymap_query\", \"logicClimate\", \"lobbymap\", \"lobbymap_stance\"}:\n",
    "    test = parse_llama(data, dataset_name)\n",
    "    test_70b = parse_llama(data_70b, dataset_name)\n",
    "    \n",
    "    if dataset_name in [\"logicClimate\", \"lobbymap_query\"]:\n",
    "        if dataset_name == \"logicClimate\":\n",
    "            y_true = test['label'].apply(literal_eval)\n",
    "            y_pred = test['model_label'].apply(lambda x: [e.strip().lower() for e in x.split(\",\")])\n",
    "        elif dataset_name == \"lobbymap_query\":\n",
    "            y_true = test['label'].apply(lambda x: [map_lobbymap_stance[e] for e in x])\n",
    "            y_pred = test['model_label'].apply(lambda x: [e.strip() for e in x.split(\",\")])   \n",
    "        \n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        if dataset_name in label_readable_mapping:\n",
    "            label2id = {v.lower(): k for k, v in label_readable_mapping[dataset_name]['labels'].items()}\n",
    "            test['model_label'] = test['model_label'].str.lower().map(label2id)\n",
    "            test_70b['model_label'] = test_70b['model_label'].str.lower().map(label2id)\n",
    "\n",
    "        if dataset_name in cot_list:\n",
    "            # print(\"worked\", dataset_name)\n",
    "            test_gpt = find_errors(dataset_name, mode=\"cot\")\n",
    "        else:\n",
    "            # print(\"not cot\", dataset_name)\n",
    "            test_gpt = find_errors(dataset_name) \n",
    "\n",
    "        # Replace non existing labels by the most frequent\n",
    "        test.loc[~test['model_label'].astype(str).isin(test['label'].astype(str).unique()), \"model_label\"] = test['label'].mode()[0]\n",
    "        test_70b.loc[~test_70b['model_label'].astype(str).isin(test_70b['label'].astype(str).unique()), \"model_label\"] = test_70b['label'].mode()[0]\n",
    "\n",
    "        error_llm = test[test['model_label'].astype(str)!=test['label'].astype(str)].copy()\n",
    "        error_llm_70b = test_70b[test_70b['model_label'].astype(str)!=test_70b['label'].astype(str)].copy()\n",
    "        error_gpt = test_gpt[test_gpt['gpt-4o-mini_label'].astype(str) != test_gpt['label'].astype(str)]\n",
    "\n",
    "        annotated_errors = pd.DataFrame()\n",
    "        if os.path.exists(path + dataset_name + \"_fp.json\"):\n",
    "            annotated_errors = pd.concat([annotated_errors, pd.read_json(path + dataset_name + \"_fp.json\")])\n",
    "        if os.path.exists(path + dataset_name + \"_fn.json\"):\n",
    "            annotated_errors = pd.concat([annotated_errors, pd.read_json(path + dataset_name + \"_fn.json\")])\n",
    "\n",
    "        if len(annotated_errors)>0:\n",
    "            if dataset_name in [\"lobbymap_stance\", \"climaQA\", 'climateFEVER_evidence']:\n",
    "                annotated_errors = annotated_errors.merge(error_llm, on=['text', 'query'], how='left')\n",
    "                annotated_errors = annotated_errors.merge(error_llm_70b, on=['text', 'query'], how='left', suffixes=(\"\", \"_70b\"))\n",
    "\n",
    "                common_gpt_70b = error_gpt.merge(error_llm_70b, how=\"inner\", on=['text', 'query'])\n",
    "                all_gpt_70b = error_gpt.merge(error_llm_70b, how=\"outer\", on=['text', 'query'])\n",
    "                prop_gpt_70b = len(common_gpt_70b)#/len(error_llm_70b)\n",
    "                prop_not_gpt_70b = (len(error_llm_70b))#-len(common_gpt_70b))#/len(error_llm_70b)\n",
    "\n",
    "                common_gpt_8b = error_gpt.merge(error_llm, how=\"inner\", on=['text', 'query'])\n",
    "                all_gpt_8b = error_gpt.merge(error_llm, how=\"outer\", on=['text', 'query'])\n",
    "                prop_gpt_8b = len(common_gpt_8b)#/len(error_llm)\n",
    "                prop_not_gpt_8b = (len(error_llm))#-len(common_gpt_8b))#/len(error_llm)\n",
    "            else:\n",
    "                annotated_errors = annotated_errors.merge(error_llm, on='text', how='left')\n",
    "                annotated_errors = annotated_errors.merge(error_llm_70b, on='text', how='left', suffixes=(\"\", \"_70b\"))\n",
    "\n",
    "                common_gpt_70b = error_gpt.merge(error_llm_70b, how=\"inner\", on=\"text\")\n",
    "                all_gpt_70b = error_gpt.merge(error_llm_70b, how=\"outer\", on=\"text\")\n",
    "                prop_gpt_70b = len(common_gpt_70b)#/len(error_llm_70b)\n",
    "                prop_not_gpt_70b = (len(error_llm_70b))#-len(common_gpt_70b))#/len(error_llm_70b)\n",
    "\n",
    "\n",
    "                common_gpt_8b = error_gpt.merge(error_llm, how=\"inner\", on=\"text\")\n",
    "                all_gpt_8b = error_gpt.merge(error_llm, how=\"outer\", on=\"text\")\n",
    "                prop_gpt_8b = len(common_gpt_8b)#/len(error_llm)\n",
    "                prop_not_gpt_8b = (len(error_llm))#-len(common_gpt_8b))#/len(error_llm)\n",
    "\n",
    "                # common_gpt_8b = error_gpt.merge(error_llm, how=\"inner\", on=\"text\")\n",
    "                # all_gpt_8b = error_gpt.merge(error_llm, how=\"outer\", on=\"text\")\n",
    "                # prop_gpt_8b = len(common_gpt_8b)/len(all_gpt_8b)\n",
    "\n",
    "            total_llama += len(annotated_errors[annotated_errors[\"model_label\"].notna()])\n",
    "            total_llama_70b += len(annotated_errors[annotated_errors[\"model_label_70b\"].notna()])\n",
    "            total_errors += len(annotated_errors)\n",
    "            total_gpt += len(error_gpt)\n",
    "\n",
    "            Total_common_8b += prop_gpt_8b\n",
    "            Total_8b += prop_not_gpt_8b\n",
    "            Total_common_70b += prop_gpt_70b\n",
    "            Total_70b += prop_not_gpt_70b\n",
    "\n",
    "\n",
    "            print(parse_dataset_name(dataset_name), \"&\",\n",
    "                len(annotated_errors[annotated_errors[\"model_label\"].notna()]), \"&\", \n",
    "                len(annotated_errors[annotated_errors[\"model_label_70b\"].notna()]), \"&\", \n",
    "                len(annotated_errors), \"&\",\n",
    "                str(int(np.round(prop_gpt_8b,0))), \"&\",\n",
    "                str(int(np.round(prop_not_gpt_8b,0))), \"&\",\n",
    "                str(int(np.round(prop_gpt_70b,0))),  \"&\",\n",
    "                str(int(np.round(prop_not_gpt_70b,0))), \"&\",\n",
    "                str(int(len(error_gpt))),\n",
    "                \"\\\\\\\\\")     \n",
    "\n",
    "print(\"\\\\midrule\")\n",
    "print(\"Total\", \"&\",\n",
    "                total_llama, \"&\", \n",
    "                total_llama_70b, \"&\", \n",
    "                total_errors, \"&\",\n",
    "                str(int(np.round(Total_common_8b,0))), \"&\",\n",
    "                str(int(np.round(Total_8b,0))), \"&\",\n",
    "                str(int(np.round(Total_common_70b,0))),  \"&\",\n",
    "                str(int(np.round(Total_70b,0))), \"&\",\n",
    "                str(int(total_gpt)),\n",
    "                \"\\\\\\\\\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
