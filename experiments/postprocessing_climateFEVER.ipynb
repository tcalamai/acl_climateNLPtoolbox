{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfb6fb1",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:04:39.963029100Z",
     "start_time": "2024-09-09T12:04:32.023907900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from experiment import load_dataset, clean_datasets, generate_args\n",
    "from src.logger import Logger\n",
    "from src.builder import DatasetBuilder\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "logger = Logger(log_filename=\"performances_climateFEVER\")\n",
    "dataset_builder = DatasetBuilder(seed=42)\n",
    "args = generate_args(dataset_builder, ['climateFEVER_evidence', 'climateFEVER_evidence_climabench', 'climateFEVER_claims'], logger)\n",
    "\n",
    "HF_REPO = \"anonymous\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da87a815802815c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:05:42.393271200Z",
     "start_time": "2024-09-09T12:05:42.371271300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_name=\"climateFEVER_evidence\"):\n",
    "    seed=42\n",
    "    dataset_max_size = 10000\n",
    "        \n",
    "    train, test, dev = load_dataset(dataset_name)\n",
    "    \n",
    "    y_train = train[args[dataset_name]['label_columns']]\n",
    "    \n",
    "    X_test = test[args[dataset_name][\"input_columns\"]]\n",
    "    y_test = test[args[dataset_name]['label_columns']]\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec3af3b03dedd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:05:44.277758600Z",
     "start_time": "2024-09-09T12:05:44.241759400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_macro_label(df):\n",
    "    # Define the logic for the macro label\n",
    "    def get_macro_label(labels):\n",
    "        labels_set = set(labels)\n",
    "        if \"SUPPORTS\" in labels_set and \"REFUTES\" in labels_set:\n",
    "            return \"DISPUTED\"\n",
    "        elif \"SUPPORTS\" in labels_set:\n",
    "            return \"SUPPORTS\"\n",
    "        elif \"REFUTES\" in labels_set:\n",
    "            return \"REFUTES\"\n",
    "        elif labels_set == {\"NOT_ENOUGH_INFO\"}:\n",
    "            return \"NOT_ENOUGH_INFO\"\n",
    "        else:\n",
    "            return \"NOT_ENOUGH_INFO\"\n",
    "\n",
    "    # Group by the claim and compute the macro label for each group\n",
    "    macro_labels = df.groupby(['claim', 'claim_label'])[\"pred_pair_label\"].apply(get_macro_label)\n",
    "\n",
    "    return macro_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22285f32",
   "metadata": {},
   "source": [
    "# Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80963e56585fadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:05:50.830145600Z",
     "start_time": "2024-09-09T12:05:50.772611800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "def evaluate_climatefever(test_dataset, model=f\"{HF_REPO}/climateFEVER_evidence_climabench_42_distilRoBERTa\", max_length=512):\n",
    "    pipe = pipeline(\"text-classification\", model=model, token=os.environ['HUB_TOKEN'],  padding=\"max_length\", truncation=True, max_length=max_length, device=0)\n",
    "\n",
    "    inputs_list = []\n",
    "    label_list = []\n",
    "    for i, r in test_dataset.iterrows():\n",
    "        inputs_list += [{'text':r['text'], 'text_pair':r['query']}]\n",
    "        label_list += [r['label']]\n",
    "        \n",
    "    outputs_list = pipe.predict(inputs_list)\n",
    "    \n",
    "    y_pred = [l['label'] for l in outputs_list]\n",
    "    test_dataset['pred_pair_label'] = y_pred\n",
    "    \n",
    "    macro_labels = compute_macro_label(test_dataset).reset_index()\n",
    "    \n",
    "    return macro_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d9039ed1fc8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:20:59.558219300Z",
     "start_time": "2024-09-09T12:20:42.254594900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = prepare_dataset(\"climateFEVER_evidence_climabench\")\n",
    "macro_labels = evaluate_climatefever(test, model=f\"{HF_REPO}/climateFEVER_evidence_climabench_42_distilRoBERTa\", max_length=512)\n",
    "logger.add_precomputed_f1_score(\n",
    "    y_test=macro_labels['claim_label'], \n",
    "    y_pred=macro_labels['pred_pair_label'],\n",
    "    dataset_name=\"climateFEVER_claim_climabench_agg\", \n",
    "    model_type=\"distilRoBERTa\", \n",
    "    n_labels=4,\n",
    ")\n",
    "logger.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d308e860bd3be6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:21:13.559835200Z",
     "start_time": "2024-09-09T12:20:59.560218200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = prepare_dataset(\"climateFEVER_evidence\")\n",
    "macro_labels = evaluate_climatefever(test, model=f\"{HF_REPO}/climateFEVER_evidence_42_distilRoBERTa\", max_length=512)\n",
    "logger.add_precomputed_f1_score(\n",
    "    y_test=macro_labels['claim_label'], \n",
    "    y_pred=macro_labels['pred_pair_label'],\n",
    "    dataset_name=\"climateFEVER_claim_agg\", \n",
    "    model_type=\"distilRoBERTa\", \n",
    "    n_labels=4,\n",
    ")\n",
    "logger.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf50cd6a9287bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:25:57.623868Z",
     "start_time": "2024-09-09T12:21:13.560835100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = prepare_dataset(\"climateFEVER_evidence\")\n",
    "\n",
    "macro_labels = evaluate_climatefever(test, model=f\"{HF_REPO}/climateFEVER_evidence_42_longformer\", max_length=4096)\n",
    "\n",
    "logger.add_precomputed_f1_score(\n",
    "    y_test=macro_labels['claim_label'], \n",
    "    y_pred=macro_labels['pred_pair_label'],\n",
    "    dataset_name=\"climateFEVER_claim_agg\", \n",
    "    model_type=\"longformer\", \n",
    "    n_labels=4,\n",
    ")\n",
    "logger.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199f0ca350e2fdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:31:39.170238Z",
     "start_time": "2024-09-09T12:25:57.625868800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = prepare_dataset(\"climateFEVER_evidence_climabench\")\n",
    "\n",
    "macro_labels = evaluate_climatefever(test, model=f\"{HF_REPO}/climateFEVER_evidence_climabench_42_longformer\", max_length=4096)\n",
    "logger.add_precomputed_f1_score(\n",
    "    y_test=macro_labels['claim_label'], \n",
    "    y_pred=macro_labels['pred_pair_label'],\n",
    "    dataset_name=\"climateFEVER_claim_climabench_agg\", \n",
    "    model_type=\"longformer\", \n",
    "    n_labels=4,\n",
    ")\n",
    "logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58429a49831e89",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Human Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb4a40b8231033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:15:18.424109800Z",
     "start_time": "2024-09-02T10:15:18.391107Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Generator\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df49324f2e6a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:00:12.755434800Z",
     "start_time": "2024-09-02T10:00:12.709436400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, dev, _ = generator.load_dataset(\"climateFEVER_evidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e29d8452837f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:38:37.154208500Z",
     "start_time": "2024-09-02T10:38:37.119150700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def parse_evidences_votes(text):\n",
    "    text=text.replace(\",\\n\", \",\")\n",
    "    pattern = r\"array\\(\\[(.*?)\\]\"\n",
    "    matches = re.search(pattern, text)\n",
    "    \n",
    "    if matches:\n",
    "        extracted_list = matches.group(1).split(', ')\n",
    "        extracted_list = [None if item == 'None' else item.strip().strip(\"'\") for item in extracted_list]\n",
    "    else:\n",
    "        extracted_list = np.nan\n",
    "    return extracted_list\n",
    "\n",
    "test['votes'] = test['evidences'].apply(parse_evidences_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202efcb2e183986",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:38:38.949969Z",
     "start_time": "2024-09-02T10:38:38.939970400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exploded_df = test.join(pd.DataFrame(test.pop('votes').tolist(), index=test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fe3d48a7df62d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:38:39.575758200Z",
     "start_time": "2024-09-02T10:38:39.553754300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exploded_df.rename(columns={\n",
    "    0:\"annotator1\",\n",
    "    1:\"annotator2\",\n",
    "    2:\"annotator3\",\n",
    "    3:\"annotator4\",\n",
    "    4:\"annotator5\",\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06fe25191adcb93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:38:40.506959100Z",
     "start_time": "2024-09-02T10:38:40.426653100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for i in [1,2,3,5]:\n",
    "    annotator = \"annotator\"+str(i)\n",
    "    \n",
    "    y_true = exploded_df[~exploded_df[annotator].isna()]['label']\n",
    "    y_pred = exploded_df[~exploded_df[annotator].isna()][annotator]\n",
    "    \n",
    "    report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=True, zero_division=0.0)\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    f1_scores += [report['macro avg']['f1-score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cc9f14c3991d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:40:05.211441800Z",
     "start_time": "2024-09-02T10:40:05.179442Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Data\n",
    "data = f1_scores\n",
    "\n",
    "# Step 1: Compute the mean\n",
    "mean = np.mean(data)\n",
    "\n",
    "# Step 2: Compute the standard deviation\n",
    "std_dev = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "# Step 3: Sample size\n",
    "n = len(data)\n",
    "\n",
    "# Step 4: Compute the Standard Error of the Mean (SEM)\n",
    "sem = std_dev / np.sqrt(n)\n",
    "\n",
    "# Step 5: Determine the confidence level (95% -> Z = 1.96)\n",
    "confidence_level = 0.95\n",
    "z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "\n",
    "# Step 6: Calculate the Margin of Error (ME)\n",
    "margin_of_error = z_score * sem\n",
    "\n",
    "# Step 7: Compute the confidence interval\n",
    "confidence_interval = (mean - margin_of_error, mean + margin_of_error)\n",
    "\n",
    "mean, confidence_interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b51bd47442c91e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a55c3482f6b89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:42:54.374656700Z",
     "start_time": "2024-09-02T10:42:54.351658600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_macro_label_annotator(df, annotator):\n",
    "    # Define the logic for the macro label\n",
    "    def get_macro_label(labels):\n",
    "        labels_set = set(labels)\n",
    "        if \"SUPPORTS\" in labels_set and \"REFUTES\" in labels_set:\n",
    "            return \"DISPUTED\"\n",
    "        elif \"SUPPORTS\" in labels_set:\n",
    "            return \"SUPPORTS\"\n",
    "        elif \"REFUTES\" in labels_set:\n",
    "            return \"REFUTES\"\n",
    "        elif labels_set == {\"NOT_ENOUGH_INFO\"}:\n",
    "            return \"NOT_ENOUGH_INFO\"\n",
    "        else:\n",
    "            return \"NOT_ENOUGH_INFO\"\n",
    "\n",
    "    # Group by the claim and compute the macro label for each group\n",
    "    macro_labels = df.groupby(['claim', 'claim_label'])[annotator].apply(get_macro_label)\n",
    "\n",
    "    return macro_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94255db7e0631633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:47:14.770427800Z",
     "start_time": "2024-09-02T10:47:14.710907Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "claims_f1 = []\n",
    "for i in [1,2,3,5]:\n",
    "    annotator = \"annotator\"+str(i)\n",
    "    \n",
    "    subset_test = exploded_df[~exploded_df[annotator].isna()].copy()\n",
    "    \n",
    "    macro_labels = compute_macro_label_annotator(subset_test, annotator).reset_index()\n",
    "    \n",
    "    report = classification_report(y_true=macro_labels['claim_label'], y_pred=macro_labels[annotator], output_dict=True, zero_division=0.0)\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    claims_f1 += [report['macro avg']['f1-score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b21a2db35b6d1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:47:17.422900100Z",
     "start_time": "2024-09-02T10:47:17.395888900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Data\n",
    "data = claims_f1\n",
    "\n",
    "# Step 1: Compute the mean\n",
    "mean = np.mean(data)\n",
    "\n",
    "# Step 2: Compute the standard deviation\n",
    "std_dev = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "# Step 3: Sample size\n",
    "n = len(data)\n",
    "\n",
    "# Step 4: Compute the Standard Error of the Mean (SEM)\n",
    "sem = std_dev / np.sqrt(n)\n",
    "\n",
    "# Step 5: Determine the confidence level (95% -> Z = 1.96)\n",
    "confidence_level = 0.95\n",
    "z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "\n",
    "# Step 6: Calculate the Margin of Error (ME)\n",
    "margin_of_error = z_score * sem\n",
    "\n",
    "# Step 7: Compute the confidence interval\n",
    "confidence_interval = (mean - margin_of_error, mean + margin_of_error)\n",
    "\n",
    "mean, confidence_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae4f47",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69632199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from zero_shot import load_dict, extract_prompt, update_question, map_lobbymap_stance, prepare_content\n",
    "\n",
    "\n",
    "# Open the JSON file\n",
    "with open(os.path.join(\"llm\", \"mappings\", \"task_description.json\"), 'r', encoding='utf-8') as file:\n",
    "    task_descriptions = json.load(file)\n",
    "\n",
    "# Open the JSON file\n",
    "with open(os.path.join(\"llm\", \"mappings\", \"label_annotation.json\"), 'r', encoding='utf-8') as file:\n",
    "    label_readable_mapping = json.load(file)\n",
    "\n",
    "prompts = load_dict(\"llm/prompts.json\")\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_label_explanation(text):\n",
    "    # Regular expression to extract Label and Explanation\n",
    "    label_pattern = r'Label:\\s*(.*)'\n",
    "    explanation_pattern = r'Explanation:\\s*(.*)'\n",
    "\n",
    "    # Find the label\n",
    "    label_match = re.search(label_pattern, text)\n",
    "    label = label_match.group(1) if label_match else None\n",
    "\n",
    "    # Find the explanation\n",
    "    explanation_match = re.search(explanation_pattern, text, re.DOTALL)\n",
    "    explanation = explanation_match.group(1).strip() if explanation_match else None\n",
    "    \n",
    "    label = label.replace('[', \"\").replace(']', \"\")\n",
    "\n",
    "    return label, explanation\n",
    "\n",
    "def find_errors(dataset_name, gpt4o=False):\n",
    "        \n",
    "    # Loading data from saved file\n",
    "    results = []\n",
    "    if gpt4o:\n",
    "        result_file_name = f\"llm/outputs/gpt-4o/{dataset_name}.jsonl\"\n",
    "    else:\n",
    "        result_file_name = f\"llm/outputs/full/{dataset_name}.jsonl\"\n",
    "        # result_file_name = f\"llm/outputs/{dataset_name}.jsonl\"\n",
    "\n",
    "\n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parsing the JSON string into a dict and appending to the list of results\n",
    "            json_object = json.loads(line.strip())\n",
    "            results.append(json_object[\"response\"]['body'][\"choices\"][0]['message']['content'])\n",
    "    \n",
    "    labels = []\n",
    "    explainations = []\n",
    "    \n",
    "    for result in results:\n",
    "        label, explanation = parse_label_explanation(result)\n",
    "        labels += [label]\n",
    "        explainations += [explanation]\n",
    "    \n",
    "    #test = pd.read_parquet(os.path.join(\"doccano\", \"random\", \"parquet\", f\"{dataset_name}.pkl\"))\n",
    "    test = pd.read_parquet(os.path.join(\"data\", \"llm_green_nlp_tasks\", f\"{dataset_name}.pkl\"))\n",
    "\n",
    "    test['gpt-4o-mini_label'] = labels\n",
    "    test['gpt-4o-mini_explanation'] = explainations\n",
    "    \n",
    "    \n",
    "    if dataset_name in label_readable_mapping:\n",
    "        label2id = {v.lower(): k for k, v in label_readable_mapping[dataset_name]['labels'].items()}\n",
    "        test['gpt-4o-mini_label'] = test['gpt-4o-mini_label'].str.lower().map(label2id)  \n",
    "        \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa499c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = find_errors(\"climateFEVER_evidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "def evaluate_climatefever(test_dataset):          \n",
    "    test_dataset['pred_pair_label'] = test[\"gpt-4o-mini_label\"].copy()\n",
    "    \n",
    "    macro_labels = compute_macro_label(test_dataset).reset_index()\n",
    "    \n",
    "    return macro_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf96b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_labels = evaluate_climatefever(test)\n",
    "logger.add_precomputed_f1_score(\n",
    "    y_test=macro_labels['claim_label'], \n",
    "    y_pred=macro_labels['pred_pair_label'],\n",
    "    dataset_name=\"climateFEVER_claim_agg\", \n",
    "    model_type=\"gpt-4o-mini\", \n",
    "    n_labels=4,\n",
    ")\n",
    "logger.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
