{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d096079f8469285",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba8f87d951b62b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:19:30.057119Z",
     "start_time": "2024-09-10T07:19:24.242356700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import Generator\n",
    "import os\n",
    "info_df = pd.read_excel(os.path.join(os.getcwd(), \"paper_utils\", \"other\", \"paper_list.xlsx\"), sheet_name=\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c597cfa070d165f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:19:31.793634700Z",
     "start_time": "2024-09-10T07:19:30.683687700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c2ba0f80db079",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c362ffaa86c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:48:07.296765Z",
     "start_time": "2024-09-10T07:48:02.280019Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizes = dict()\n",
    "\n",
    "for dataset_name in set(generator.dataset_list)-set([\"ClimaINS_ours\", \"climateFEVER_evidence_climabench\"]):\n",
    "    # print(dataset_name)\n",
    "    train, test, dev = generator.loading_raw_datasets(dataset_name)\n",
    "    # print(\"loaded\")\n",
    "    df = pd.concat([train, test, dev])\n",
    "    mx = df['token_counts'].max()\n",
    "    df = 0\n",
    "\n",
    "    sizes[dataset_name] = {\n",
    "        \"train\": len(train),\n",
    "        \"test\": len(test),\n",
    "        \"dev\": len(dev),\n",
    "        \"token\": mx\n",
    "    }\n",
    "    \n",
    "sorted_by_values = dict(sorted(sizes.items(), key=lambda item: item[1][\"test\"]))\n",
    "    \n",
    "for dataset_name in sorted_by_values.keys():\n",
    "    line = \"\"\n",
    "    line += dataset_name.replace(\"_\", \"\\\\_\").replace(\"&\", \"\\\\&\") + info_df[info_df[\"name\"]==dataset_name][\"source\"].values[0].replace(\"citet\", \"cite\") + \" & \"\n",
    "    line += str(sorted_by_values[dataset_name][\"train\"]) + \" & \"\n",
    "    line += str(sorted_by_values[dataset_name][\"dev\"]) + \" & \"\n",
    "    line += str(sorted_by_values[dataset_name][\"test\"]) + \" & \"\n",
    "    line += str(sorted_by_values[dataset_name][\"token\"]) + \"\\\\\\\\\"\n",
    "\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767be515fc5bc335",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dataset Imbalanced Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9df3e9cf1bb57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:48:32.736010Z",
     "start_time": "2024-09-10T07:48:32.715776500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb49a64a7764a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:48:33.608397Z",
     "start_time": "2024-09-10T07:48:33.582384800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def imbalance_ratio(df, label_column):\n",
    "    counter = Counter(df[label_column])\n",
    "    majority_class_count = max(counter.values())\n",
    "    minority_class_count = min(counter.values())\n",
    "    imbalance_ratio = majority_class_count / minority_class_count\n",
    "    return imbalance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151872854d95743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:48:54.256729400Z",
     "start_time": "2024-09-10T07:48:49.121861800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imbalance_ratios = dict()\n",
    "test_imbalance_ratios = dict()\n",
    "raw_imbalance_ratios = dict()\n",
    "summary_count_values = dict()\n",
    "\n",
    "for dataset_name in set(generator.dataset_list)-set([\"ClimaINS_ours\", \"climateFEVER_evidence_climabench\"]):\n",
    "    print(dataset_name)\n",
    "    raw_train, _, _ = generator.loading_raw_datasets(dataset_name)\n",
    "    train, test, _, _ = generator.load_nlp_tasks_dataset(dataset_name)\n",
    "    \n",
    "    if dataset_name == \"logicClimate\":\n",
    "        raw_train['label'] = raw_train['label'].apply(ast.literal_eval)\n",
    "        raw_train = raw_train[['label']].explode('label')\n",
    "        test['label'] = test['label'].apply(ast.literal_eval)\n",
    "        test = test[['label']].explode('label')\n",
    "        train['label'] = train['label'].apply(ast.literal_eval)\n",
    "        train = train[['label']].explode('label')\n",
    "    elif dataset_name == \"lobbymap_query\":\n",
    "        raw_train = raw_train[['label']].explode('label')\n",
    "        test = test[['label']].explode('label')\n",
    "        train = train[['label']].explode('label')\n",
    "\n",
    "    raw_imbalance_ratios[dataset_name] = imbalance_ratio(raw_train, generator.args[dataset_name]['label_columns'])\n",
    "    imbalance_ratios[dataset_name] = imbalance_ratio(train, generator.args[dataset_name]['label_columns'])\n",
    "    test_imbalance_ratios[dataset_name] = imbalance_ratio(test, generator.args[dataset_name]['label_columns'])\n",
    "    summary_count_values[dataset_name] = raw_train[generator.args[dataset_name]['label_columns']].value_counts()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7527547bdb4b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:49:04.038802900Z",
     "start_time": "2024-09-10T07:49:03.991274500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_by_values = dict(sorted(raw_imbalance_ratios.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066ad4cf9bfa58b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:49:05.229555100Z",
     "start_time": "2024-09-10T07:49:05.170550300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dataset_name in sorted_by_values.keys():\n",
    "    line = \"\"\n",
    "    line += dataset_name.replace(\"_\", \"\\\\_\").replace(\"&\", \"\\\\&\") + info_df[info_df[\"name\"]==dataset_name][\"source\"].values[0].replace(\"citet\", \"cite\") + \" & \"\n",
    "    line += str(np.round(raw_imbalance_ratios[dataset_name],1)) + \" & \"\n",
    "    line += str(np.round(imbalance_ratios[dataset_name],1)) + \" & \"\n",
    "    line += str(np.round(test_imbalance_ratios[dataset_name],1)) + \" & \"\n",
    "    if generator.args[dataset_name][\"weighted_loss\"]:\n",
    "        line += \"\\\\cmark\"\n",
    "    line += \" \\\\\\\\\"\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbe1a450d1b074",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dataset Language and Noise Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd1bf1489c8df1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:53:39.564300500Z",
     "start_time": "2024-09-10T07:53:39.465780500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, dev = generator.loading_raw_datasets(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82dda9e176314c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:53:44.433413700Z",
     "start_time": "2024-09-10T07:53:39.837658700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"dataset & non-english & non-clean & noise & word salad \\\\\")\n",
    "\n",
    "for dataset_name in set(generator.dataset_list)-set([\"ClimaINS_ours\", \"climateFEVER_evidence_climabench\"]):\n",
    "    train, test, dev = generator.loading_raw_datasets(dataset_name)\n",
    "    \n",
    "    line = \"\"\n",
    "    \n",
    "    line += dataset_name.replace(\"_\", \"\\\\_\").replace(\"&\", \"\\\\&\")  + info_df[info_df[\"name\"]==dataset_name][\"source\"].values[0].replace(\"citet\", \"cite\") + \" & \"\n",
    "    line += str(np.round(100*(train[\"language\"].value_counts().sum() - train[\"language\"].value_counts()['en']) / train[\"language\"].value_counts().sum(), 2)) + \" \\\\%  & \"\n",
    "    line += str(np.round(100*(train[\"gibberish\"].value_counts().sum() - train[\"gibberish\"].value_counts()['clean']) / train[\"gibberish\"].value_counts().sum(), 2)) + \" \\\\%  & \"\n",
    "    line += str(np.round(100*(train[\"gibberish\"].value_counts()['noise'] if \"noise\" in train[\"gibberish\"].value_counts() else 0) / train[\"gibberish\"].value_counts().sum(), 2)) + \" \\\\%  & \"\n",
    "    line += str(np.round(100*(train[\"gibberish\"].value_counts()['word salad'] if \"word salad\" in train[\"gibberish\"].value_counts() else 0) / train[\"gibberish\"].value_counts().sum(), 2)) + \" \\\\%  \\\\\\\\\"\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6915131f10949",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Duplicates and contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731479204b43835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:29:24.297613900Z",
     "start_time": "2024-09-10T09:29:23.628270Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, dev = generator.loading_raw_datasets(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2b9abf066e82a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:37:04.452256600Z",
     "start_time": "2024-09-10T09:37:04.397258300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb2194385c8752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:41:48.764071500Z",
     "start_time": "2024-09-10T09:41:43.152993100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"dataset & text duplicates & cleaned text duplicates & exact duplicates & Dataset Size \\\\\\\\\")\n",
    "\n",
    "for dataset_name in set(generator.dataset_list)-set([\"ClimaINS_ours\", \"climateFEVER_climabench\", \"lobbymap_query\"]):\n",
    "    train = pd.read_parquet(\"data/cleaned_datasets_archive/\"+dataset_name+\"/train.pkl\") \n",
    "    test = pd.read_parquet(\"data/cleaned_datasets_archive/\"+dataset_name+\"/test.pkl\") \n",
    "    dev = pd.read_parquet(\"data/cleaned_datasets_archive/\"+dataset_name+\"/dev.pkl\")\n",
    "        \n",
    "    full_dataset = pd.concat([train, test, dev], ignore_index=True)\n",
    "    \n",
    "    if dataset_name == \"climateFEVER_evidence_climabench\":\n",
    "        dataset_name = \"climateFEVER_evidence\"\n",
    "\n",
    "    print(\n",
    "        dataset_name.replace(\"_\", \"\\\\_\").replace(\"&\", \"\\\\&\")  + info_df[info_df[\"name\"]==dataset_name][\"source\"].values[0].replace(\"citet\", \"cite\"), \"&\",\n",
    "        len(full_dataset[full_dataset.duplicated(subset=[\"text\"], keep=False)]), \"&\",\n",
    "        len(full_dataset[full_dataset.duplicated(subset=[\"clean_text\"], keep=False)]), \"&\",\n",
    "        len(full_dataset[full_dataset.duplicated(subset=[\"clean_text\", \"label\"], keep=False)]), \"&\",\n",
    "        len(full_dataset),\n",
    "        \"\\\\\\\\\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75b88318dc0acd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    line = \"\"\n",
    "    \n",
    "    line += dataset_name.replace(\"_\", \"\\\\_\").replace(\"&\", \"\\\\&\")  + info_df[info_df[\"name\"]==dataset_name][\"source\"].values[0].replace(\"citet\", \"cite\") + \" & \"\n",
    "    line += str(np.round(100*(train[\"language\"].value_counts().sum() - train[\"language\"].value_counts()['en']) / train[\"language\"].value_counts().sum(), 2)) + \" \\\\%  & \"\n",
    "    line += str(np.round(100*(train[\"gibberish\"].value_counts().sum() - train[\"gibberish\"].value_counts()['clean']) / train[\"gibberish\"].value_counts().sum(), 2)) + \" \\\\%  & \"\n",
    "    line += str(np.round(100*(train[\"gibberish\"].value_counts()['noise'] if \"noise\" in train[\"gibberish\"].value_counts() else 0) / train[\"gibberish\"].value_counts().sum(), 2)) + \" \\\\%  & \"\n",
    "    line += str(np.round(100*(train[\"gibberish\"].value_counts()['word salad'] if \"word salad\" in train[\"gibberish\"].value_counts() else 0) / train[\"gibberish\"].value_counts().sum(), 2)) + \" \\\\%  \\\\\\\\\"\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff9589d3c1437f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T15:40:01.424535600Z",
     "start_time": "2024-09-09T15:40:01.112730300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, dev = generator.loading_raw_datasets(\"climatext\")\n",
    "\n",
    "train[train[\"gibberish\"] == \"noise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555956d9b8b0ddd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:43:40.518630500Z",
     "start_time": "2024-09-10T07:43:37.035034100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from experiment import load_dataset\n",
    "\n",
    "path = os.path.join(os.getcwd(), \"experiment_results\", \"cartography\", 'distilRoBERTa')\n",
    "\n",
    "# Collect all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(path) if file.endswith('.tsv')]\n",
    "new_csv_files = []\n",
    "for dataset_name in generator.dataset_list:\n",
    "    if dataset_name+\"_train_42.tsv\" in csv_files:\n",
    "        new_csv_files += [(dataset_name, os.path.join(path, dataset_name+\"_train_42.tsv\"))]\n",
    "csv_files=new_csv_files\n",
    "\n",
    "for idx, (dataset_name, file) in enumerate(csv_files):\n",
    "    print(\"Processing file: \", dataset_name)\n",
    "    \n",
    "    train, _, _ = load_dataset(dataset_name)\n",
    "    \n",
    "    carto_df = pd.read_csv(file, sep=\"\\t\")\n",
    "    \n",
    "    print(len(carto_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25860fab5b1561",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:45:16.214612400Z",
     "start_time": "2024-09-10T07:45:14.536940800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dataset_name in generator.dataset_list:\n",
    "    train , test, dev, _ = generator.load_nlp_tasks_dataset(dataset_name)\n",
    "    print(\"Processing file: \", dataset_name)\n",
    "    print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a09373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataset & text duplicates & exact duplicates & Dataset Size \\\\\\\\\")\n",
    "\n",
    "for dataset_name in set(generator.dataset_list)-set([\"ClimaINS_ours\", \"climateFEVER_climabench\", \"lobbymap_query\"]):\n",
    "    train = pd.read_parquet(\"data/green_nlp_tasks/\"+dataset_name+\"/train.pkl\") \n",
    "    test = pd.read_parquet(\"data/green_nlp_tasks/\"+dataset_name+\"/test.pkl\") \n",
    "    dev = pd.read_parquet(\"data/green_nlp_tasks/\"+dataset_name+\"/dev.pkl\")\n",
    "        \n",
    "    full_dataset = pd.concat([train, test, dev], ignore_index=True)\n",
    "    \n",
    "    if dataset_name == \"climateFEVER_evidence_climabench\":\n",
    "        dataset_name = \"climateFEVER_evidence\"\n",
    "\n",
    "    print(\n",
    "        dataset_name.replace(\"_\", \"\\\\_\").replace(\"&\", \"\\\\&\")  + info_df[info_df[\"name\"]==dataset_name][\"source\"].values[0].replace(\"citet\", \"cite\"), \"&\",\n",
    "        len(full_dataset[full_dataset.duplicated(subset=[\"text\"], keep=False)]), \"&\",\n",
    "        len(full_dataset[full_dataset.duplicated(subset=[\"text\", \"label\"], keep=False)]), \"&\",\n",
    "        len(full_dataset),\n",
    "        \"\\\\\\\\\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
